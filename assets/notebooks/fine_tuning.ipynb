{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/assets\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Softmax,\n",
    "    Dense,\n",
    "    AdditiveAttention,\n",
    "    MultiHeadAttention,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding\n",
    ")\n",
    "\n",
    "from tensorflow.keras import (\n",
    "    Sequential,\n",
    "    Model\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEts do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../results/PreTrainedModel/ were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../results/PreTrainedModel/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "new_model = TFBertModel.from_pretrained(\"../results/PreTrainedModel/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../results/PreTrainedModel/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"2021-01-29 18:07:09 INFO  HealthCheckService:106 - status: HEALTHY, name: render /ruleset/retrieveAllRulesets, duration_ms: 2, failure_reason: none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2 = \"2021-01-29 18:07:19.834 INFO  (qtp1991278377-18) [c:catalog s:shard1 r:core_node3 x:catalog_shard1_replica_n1] o.a.s.u.p.LogUpdateProcessorFactory [catalog_shard1_replica_n1]  webapp=/solr path=/update params={wt=javabin&version=2}{add=[ffeef73262fd4957989d0835d2c64cb0 (1690245413032951808), 00311ec2b68445cfa7bd6b94bb3f8868 (1690245413036097536), 809c0dd680da476fa20bccb21e7a791f (1690245413038194688), c7ab97f8c8194adaad5268a04c01fd4c (1690245413041340416), 1e68fda379694be98c5731c96c3becfc (1690245413044486144), dd1c48d436c74056b9d5329b958cdfb6 (1690245413048680448), c547b31c7250473998b6c72a8cdc3e41 (1690245413073846272), 4429838b832b4a98869ee7cfc84e4520 (1690245413075943424), 23b29fba566c4b19b3b8a1da7901462f (1690245413080137728), 393753568bdf42cda367643d3e87a3c3 (1690245413088526336), ... (250 adds)]} 0 1000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  819,\n",
       "  0,\n",
       "  523,\n",
       "  0,\n",
       "  420,\n",
       "  7,\n",
       "  66,\n",
       "  16,\n",
       "  859,\n",
       "  16,\n",
       "  6,\n",
       "  61,\n",
       "  834,\n",
       "  242,\n",
       "  16,\n",
       "  233,\n",
       "  0,\n",
       "  112,\n",
       "  16,\n",
       "  222,\n",
       "  0,\n",
       "  236,\n",
       "  16,\n",
       "  519,\n",
       "  0,\n",
       "  1501,\n",
       "  0,\n",
       "  1892,\n",
       "  0,\n",
       "  0,\n",
       "  16,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  16,\n",
       "  279,\n",
       "  2,\n",
       "  819,\n",
       "  0,\n",
       "  523,\n",
       "  0,\n",
       "  420,\n",
       "  7,\n",
       "  66,\n",
       "  16,\n",
       "  859,\n",
       "  16,\n",
       "  524,\n",
       "  0,\n",
       "  14,\n",
       "  75,\n",
       "  62,\n",
       "  834,\n",
       "  0,\n",
       "  35,\n",
       "  391,\n",
       "  70,\n",
       "  61,\n",
       "  683,\n",
       "  64,\n",
       "  561,\n",
       "  75,\n",
       "  69,\n",
       "  69,\n",
       "  0,\n",
       "  7,\n",
       "  66,\n",
       "  0,\n",
       "  0,\n",
       "  21,\n",
       "  16,\n",
       "  143,\n",
       "  37,\n",
       "  16,\n",
       "  129,\n",
       "  36,\n",
       "  16,\n",
       "  0,\n",
       "  42,\n",
       "  16,\n",
       "  0,\n",
       "  0,\n",
       "  33,\n",
       "  0,\n",
       "  19,\n",
       "  0,\n",
       "  37,\n",
       "  0,\n",
       "  39,\n",
       "  0,\n",
       "  34,\n",
       "  0,\n",
       "  520,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  375,\n",
       "  0,\n",
       "  133,\n",
       "  371,\n",
       "  0,\n",
       "  186,\n",
       "  372,\n",
       "  0,\n",
       "  332,\n",
       "  0,\n",
       "  370,\n",
       "  0,\n",
       "  331,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  297,\n",
       "  0,\n",
       "  1991,\n",
       "  0,\n",
       "  1869,\n",
       "  0,\n",
       "  1969,\n",
       "  0,\n",
       "  1870,\n",
       "  0,\n",
       "  1983,\n",
       "  0,\n",
       "  1868,\n",
       "  0,\n",
       "  1986,\n",
       "  0,\n",
       "  1873,\n",
       "  0,\n",
       "  1976,\n",
       "  0,\n",
       "  1871,\n",
       "  0,\n",
       "  1989,\n",
       "  0,\n",
       "  1872,\n",
       "  0,\n",
       "  1966,\n",
       "  0,\n",
       "  1886,\n",
       "  0,\n",
       "  1978,\n",
       "  0,\n",
       "  1885,\n",
       "  0,\n",
       "  1972,\n",
       "  0,\n",
       "  1832,\n",
       "  0,\n",
       "  1967,\n",
       "  0,\n",
       "  1831,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  423,\n",
       "  500,\n",
       "  0,\n",
       "  6,\n",
       "  868,\n",
       "  2],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(log, log2)\n",
    "x = dict(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1065/2125329389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m new_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assets/libs/transformers/src/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n\u001b[0;32m--> 872\u001b[0;31m         inputs = input_processing(\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assets/libs/transformers/src/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36minput_processing\u001b[0;34m(func, config, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparameter_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparameter_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "new_model(\n",
    "    input_ids=x[\"input_ids\"],\n",
    "    attention_mask=x[\"attention_mask\"],\n",
    "    token_type_ids=x[\"token_type_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerGlobalConfig:\n",
    "    d_model: int = 512\n",
    "    max_seq_length: int = 200\n",
    "    global_training: bool = True\n",
    "    storage_path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BERTLayerConfig:\n",
    "    num_attention_heads: int = 8\n",
    "    num_encoder_layers: int = 12\n",
    "    dff: int = 2048\n",
    "    max_seq_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    load_model: bool = False\n",
    "    save_model: bool = True\n",
    "    training: bool = True\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HitAnomalyLayerConfig:\n",
    "    num_attention_heads: int = 12\n",
    "    num_encoder_layers: int = 3\n",
    "    dff: int = 2048\n",
    "    max_seq_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    load_model: bool = False\n",
    "    save_model: bool = True\n",
    "    training: bool = True\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self):\n",
    "        self._global = TransformerGlobalConfig()\n",
    "        self.BERT = BERTLayerConfig()\n",
    "        self.HitAnomaly = HitAnomalyLayerConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                transformer_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self._global.load(transformer_config)\n",
    "        self.BERT.load(transformer_config)\n",
    "        self.HitAnomaly.load(transformer_config)\n",
    "\n",
    "def set_attributes_from_object(self, *args):\n",
    "    try:\n",
    "        for obj in args:\n",
    "            for attr_key, attr in obj.__dict__.items():\n",
    "                setattr(self, attr_key, attr)\n",
    "    except Exception as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model: int, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEncodingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1  # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dff: int,\n",
    "        rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.multi_headed_attention = MultiHeadAttention(num_heads=num_heads,\n",
    "                                                         key_dim=d_model // num_heads,\n",
    "                                                         dropout=0.1,\n",
    "                                                         attention_axes=(1))\n",
    "\n",
    "        self.feed_forward_network = Sequential([\n",
    "            Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "            Dense(d_model, activation='relu')  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        # (1) - Attention Score\n",
    "#         logger.info('MULTIHEADED ATTENTION')\n",
    "#         logger.info(x.shape)\n",
    "        attn_output, attn_weights = self.multi_headed_attention(\n",
    "            x,\n",
    "            x,\n",
    "            return_attention_scores=True)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (2) - Add & Normalize\n",
    "        attn_output = self.dropout1(attn_output, training=True)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (3) - Feed Forward NN\n",
    "        feed_forward_output = self.feed_forward_network(out1)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (4) - Add & Normalize\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=True)\n",
    "        out2 = self.layernorm2(out1 + feed_forward_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return tf.convert_to_tensor(out2), tf.convert_to_tensor(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTLayer(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_config: TransformerGlobalConfig,\n",
    "        config: BERTLayerConfig):\n",
    "        super(BERTLayer, self).__init__()\n",
    "\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            global_config,\n",
    "            config)\n",
    "\n",
    "        self.bert_layer_blocks = [EncoderBlock(\n",
    "            self.d_model,\n",
    "            self.num_attention_heads,\n",
    "            self.dff,\n",
    "            rate=self.dropout_rate) for _ in range(self.num_encoder_layers)]\n",
    "\n",
    "    def call(self, input_: tf.tuple, **kwargs):\n",
    "        enc_input = input_[0]\n",
    "#         logger.info('BERT LAYER')\n",
    "#         logger.info(enc_input.shape)\n",
    "        encoding_padding_mask = None\n",
    "        # BERT for Log Sequence Embedding\n",
    "        for layer_idx in range(self.num_encoder_layers):\n",
    "#             logger.info('BERT LAYER LOOP')\n",
    "#             logger.info(enc_input.shape)\n",
    "            enc_output, attention = self.bert_layer_blocks[layer_idx](enc_input, encoding_padding_mask)\n",
    "            bert_ret = tf.tuple(enc_output, attention)\n",
    "        return bert_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HitAnomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HitAnomalyLayer(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        global_config: TransformerGlobalConfig,\n",
    "        config: HitAnomalyLayerConfig):\n",
    "        super(HitAnomalyLayer, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            global_config,\n",
    "            config)\n",
    "\n",
    "        self.encoding_blocks = [EncoderBlock(\n",
    "            self.d_model,\n",
    "            self.num_attention_heads,\n",
    "            self.dff,\n",
    "            rate=self.dropout_rate\n",
    "        ) for _ in range(self.num_encoder_layers)]\n",
    "\n",
    "        self.hidden_layer_output = []\n",
    "\n",
    "#     @tf.function(jit_compile=True)\n",
    "    def call(self, input_: tf.tuple, **kwargs):\n",
    "        enc_input = input_[0]\n",
    "        encoding_padding_mask = None\n",
    "\n",
    "        # Encoder Block Hidden Layers for Log Encoder\n",
    "        # (batch_size, inp_seq_len, d_model), (batch_size, class, inp_seq_len, inp_seq_len)\n",
    "        for layer_idx in range(self.num_encoder_layers - 1):\n",
    "            enc_output, att = self.encoding_blocks[layer_idx](enc_input, encoding_padding_mask)\n",
    "            self.hidden_layer_output.append(enc_output)\n",
    "\n",
    "        fin_output = enc_output\n",
    "        final_output = tf.reduce_mean(fin_output, axis=1)\n",
    "        final_output = tf.expand_dims(final_output, axis=0)\n",
    "\n",
    "        # Last Encoding Block for Log Sequence Representation\n",
    "        out, att = self.encoding_blocks[self.num_encoder_layers - 1](final_output, encoding_padding_mask)\n",
    "        self.hidden_layer_output.append(out)\n",
    "\n",
    "        # Final Pooling Layer\n",
    "        seq_representation = tf.reduce_mean(out, axis=1)\n",
    "\n",
    "        return seq_representation, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PrimeTokenizer,\n",
    "        config: TransformerConfig):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            config._global)\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            self.vocab_size,\n",
    "            self.d_model,\n",
    "            input_length=self.max_seq_len)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            self.max_seq_len,\n",
    "            self.d_model)\n",
    "\n",
    "        self.bert_layer = BERTLayer(\n",
    "            config._global,\n",
    "            config.BERT)\n",
    "\n",
    "        self.hitanomaly_layer = HitAnomalyLayer(\n",
    "            self.vocab_size,\n",
    "            config._global,\n",
    "            config.HitAnomaly)\n",
    "\n",
    "        #self.dropout = Dropout(rate)\n",
    "\n",
    "#     @tf.function(jit_compile=True)\n",
    "    def call(self, input_tuple: tf.tuple, **kwargs):\n",
    "        log_batch = input_tuple[0]\n",
    "#         logger.info('INITIAL')\n",
    "#         logger.info(log_batch.shape)\n",
    "        encoding_padding_mask = None # input_tuple[1]\n",
    "\n",
    "        embedding_tensor = self.embedding(log_batch) # (batch_size, input_seq_len, d_model)\n",
    "#         logger.info('POST EMBEDDING LAYER')\n",
    "#         logger.info(embedding_tensor.shape)\n",
    "\n",
    "        embedding_tensor = self.pos_encoding(embedding_tensor)\n",
    "#         logger.info('POST POSITIONAL ENCODING')\n",
    "#         logger.info(embedding_tensor.shape)\n",
    "        #embedding_tensor = self.dropout(embedding_tensor, training=TRAINING)\n",
    "\n",
    "        # BERT for Log Sequence Embedding\n",
    "        bert_arg = tf.tuple(embedding_tensor, encoding_padding_mask)\n",
    "        bert_ret = self.bert_layer(bert_arg)\n",
    "\n",
    "        # Encoder Block Hidden Layers for Log Sequence Representation\n",
    "#         seq_representation, att = self.hitanomaly_layer(tf.tuple(enc_output, encoding_padding_mask))\n",
    "\n",
    "\n",
    "        return bert_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# -- Transformer Model -- #\n",
    "transformer_config_path = SOURCE + '/assets/notebooks/TransformerConfig.yaml'\n",
    "transformer_config = TransformerConfig()\n",
    "transformer_config.load(transformer_config_path)\n",
    "# optimus_prime = Transformer(prime_tokenizer, transformer_config)\n",
    "\n",
    "t_config = transformer_config._global\n",
    "\n",
    "# -- Pipeline Info -- #\n",
    "attns = []\n",
    "\n",
    "# -- Data Batches -- #\n",
    "# batched_dataset = process_all_batches(n_iter, log_labels, t_config.batch_size)\n",
    "\n",
    "# -- Model Metrics -- #\n",
    "learning_rate = CustomSchedule(t_config.d_model)\n",
    "epoch_loss = Mean(name='train_loss')\n",
    "epoch_accuracy = Mean(name='train_accuracy')\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# -- Classification Step Layers -- #\n",
    "add_att_layer = AdditiveAttention()\n",
    "softmax = Softmax()\n",
    "s1 = Sequential([\n",
    "    Dense(t_config.batch_size, activation=t_config.activation),\n",
    "    Dense(4, activation=t_config.activation),\n",
    "    Softmax()\n",
    "])\n",
    "\n",
    "# -- Checkpoints -- #\n",
    "# checkpoint_path = SOURCE + \"checkpoints/\"\n",
    "# checkpoint = Checkpoint(step=tf.Variable(1), transformer=optimus_prime, optimizer=optimizer)\n",
    "# checkpoint_manager = CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "writer = tf.summary.create_file_writer(SOURCE + t_config.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = transformer_config._global.batch_size\n",
    "max_seq_len = transformer_config._global.max_seq_len\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=([batch_size, max_seq_len]), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=([batch_size]), dtype=tf.int8)\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature,\n",
    "             experimental_compile=True)\n",
    "def train_step(log_batch: tf.Tensor, labels: tf.Tensor):\n",
    "\n",
    "    transformer_input = tf.tuple([\n",
    "        log_batch,  # <tf.Tensor: shape=(batch_size, max_seq_len), dtype=int32>\n",
    "        labels  # <tf.Tensor: shape=(batch_size, num_classes), dtype=float32>\n",
    "    ])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        transformer_ret = optimus_prime(transformer_input)\n",
    "#         a_s = add_att_layer([Rs, Rs])\n",
    "#         y = softmax(a_s * Rs)\n",
    "#         print(a_s.shape)\n",
    "        # y = Rs\n",
    "#         loss = tf.py_function(loss_function, [labels, y], tf.float32)\n",
    "#         pred = s1(y)\n",
    "#         labels = tf.cast(labels, tf.int8)\n",
    "    # Optimize the model\n",
    "#     grads = tape.gradient(loss, optimus_prime.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, optimus_prime.trainable_variables))\n",
    "\n",
    "#     acc = accuracy_function(labels, pred)\n",
    "\n",
    "    # Tracking Progress\n",
    "#     epoch_loss.update_state(loss)  # Adding Batch Loss\n",
    "#     epoch_accuracy.update_state(acc)\n",
    "\n",
    "    return transformer_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_step_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attentions = []\n",
    "\n",
    "for epoch in range(t_config.epoch):\n",
    "\n",
    "    start = time.time()\n",
    "    epoch_loss.reset_states()\n",
    "    epoch_accuracy.reset_states()\n",
    "    dataset_iter = iter(batched_dataset)\n",
    "\n",
    "    t = tqdm(range(n_iter), desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%}\".format(0, 0, 0), position=0, leave=True)\n",
    "    for _ in t:\n",
    "        batch = next(dataset_iter)\n",
    "        log_batch = batch[0]\n",
    "        labels = batch[1]\n",
    "\n",
    "        # Returns Eager Tensor for Predictions\n",
    "#         tf.summary.trace_on()\n",
    "#         tf.profiler.experimental.start(SOURCE + t_config.logdir)\n",
    "\n",
    "#         with writer.as_default():\n",
    "        transforer_ret = train_step(log_batch, labels)\n",
    "        attentions.append(transformer_ret)\n",
    "          # with tf.summary.record_if(True):\n",
    "\n",
    "#             tf.summary.trace_export(\n",
    "#               name = \"training_trace\",\n",
    "#               step=0,\n",
    "#               profiler_outdir=SOURCE + t_config.logdir\n",
    "#             )\n",
    "\n",
    "#         tf.profiler.experimental.stop()\n",
    "#         tf.summary.trace_off()\n",
    "\n",
    "#         checkpoint.step.assign_add(1)\n",
    "\n",
    "#         if int(checkpoint.step) % 10 == 0:\n",
    "#             save_path = checkpoint_manager.save()\n",
    "\n",
    "        t.set_description(desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%} \".format(epoch,\n",
    "                                                                    epoch_loss.result(),\n",
    "                                                                    epoch_accuracy.result()))\n",
    "        t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(acc, SOURCE + '/results/attention.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(Rs, SOURCE + '/results/Rs.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
