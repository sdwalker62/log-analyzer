{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/assets\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Softmax,\n",
    "    Dense,\n",
    "    AdditiveAttention,\n",
    "    MultiHeadAttention,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding\n",
    ")\n",
    "\n",
    "from tensorflow.keras import (\n",
    "    Sequential,\n",
    "    Model\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lets do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../results/PreTrainedModel/ were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../results/PreTrainedModel/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "new_model = TFBertModel.from_pretrained(\"../results/PreTrainedModel/\", local_files_only=True)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../results/PreTrainedModel/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"INFO  HealthCheckService:106 - status: HEALTHY, name: render /ruleset/retrieveAllRulesets, duration_ms: 2, failure_reason: none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2 = \"INFO  (qtp1991278377-18) [c:catalog s:shard1 r:core_node3 x:catalog_shard1_replica_n1] o.a.s.u.p.LogUpdateProcessorFactory [catalog_shard1_replica_n1]  webapp=/solr path=/update params={wt=javabin&version=2}{add=[ffeef73262fd4957989d0835d2c64cb0 (1690245413032951808), 00311ec2b68445cfa7bd6b94bb3f8868 (1690245413036097536), 809c0dd680da476fa20bccb21e7a791f (1690245413038194688), c7ab97f8c8194adaad5268a04c01fd4c (1690245413041340416), 1e68fda379694be98c5731c96c3becfc (1690245413044486144), dd1c48d436c74056b9d5329b958cdfb6 (1690245413048680448), c547b31c7250473998b6c72a8cdc3e41 (1690245413073846272), 4429838b832b4a98869ee7cfc84e4520 (1690245413075943424), 23b29fba566c4b19b3b8a1da7901462f (1690245413080137728), 393753568bdf42cda367643d3e87a3c3 (1690245413088526336), ... (250 adds)]} 0 1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame({\"logs\": [log, log2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(\n",
    "    dt[\"logs\"].tolist(),\n",
    "    padding='max_length',\n",
    "    max_length=300,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "x = dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(2, 300, 512), dtype=float32, numpy=\n",
       "array([[[ 1.5100684 ,  0.81711674, -1.8835433 , ...,  0.33733317,\n",
       "          1.5984166 ,  0.19801913],\n",
       "        [-1.4137377 ,  2.0870078 , -0.35905617, ...,  0.6584699 ,\n",
       "         -0.36399263, -1.2063018 ],\n",
       "        [ 0.8806996 ,  0.7416481 , -0.16320974, ...,  0.18457912,\n",
       "         -1.4883114 ,  0.3758193 ],\n",
       "        ...,\n",
       "        [ 1.4394886 ,  0.6448962 , -0.90818644, ..., -0.38945514,\n",
       "          0.370754  , -1.6600301 ],\n",
       "        [ 1.4173337 ,  0.9892787 , -0.6283121 , ...,  0.31201038,\n",
       "         -0.39546224, -1.9692696 ],\n",
       "        [ 0.7004079 ,  0.63683754, -0.46776372, ...,  0.03642247,\n",
       "          0.62092674, -3.3203356 ]],\n",
       "\n",
       "       [[ 1.4250323 ,  0.80526364, -1.7922479 , ...,  0.22904322,\n",
       "          1.6746545 ,  0.2019776 ],\n",
       "        [-1.4716982 ,  2.1031938 , -0.24442998, ...,  0.5462447 ,\n",
       "         -0.29639524, -1.2052654 ],\n",
       "        [ 0.4133391 ,  0.7002309 , -0.74047464, ...,  1.0383904 ,\n",
       "          0.4717614 , -0.6072141 ],\n",
       "        ...,\n",
       "        [ 1.352105  ,  0.65947175, -0.8076258 , ..., -0.533981  ,\n",
       "          0.4497779 , -1.6472658 ],\n",
       "        [ 1.3298956 ,  0.996977  , -0.5250914 , ...,  0.18784164,\n",
       "         -0.32643136, -1.9559304 ],\n",
       "        [ 0.5995828 ,  0.63120973, -0.38008368, ..., -0.08085719,\n",
       "          0.69551677, -3.3051581 ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[-0.44702232, -0.6855663 ,  0.5165402 , ..., -0.04094591,\n",
       "        -0.4136328 ,  0.15797558],\n",
       "       [-0.47060257, -0.6474663 ,  0.555997  , ..., -0.00531548,\n",
       "        -0.4485737 ,  0.17429526]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model(x, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerGlobalConfig:\n",
    "    d_model: int = 512\n",
    "    max_seq_length: int = 200\n",
    "    global_training: bool = True\n",
    "    storage_path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BERTLayerConfig:\n",
    "    num_attention_heads: int = 8\n",
    "    num_encoder_layers: int = 12\n",
    "    dff: int = 2048\n",
    "    max_seq_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    load_model: bool = False\n",
    "    save_model: bool = True\n",
    "    training: bool = True\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HitAnomalyLayerConfig:\n",
    "    num_attention_heads: int = 12\n",
    "    num_encoder_layers: int = 3\n",
    "    dff: int = 2048\n",
    "    max_seq_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "    load_model: bool = False\n",
    "    save_model: bool = True\n",
    "    training: bool = True\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self):\n",
    "        self._global = TransformerGlobalConfig()\n",
    "        self.BERT = BERTLayerConfig()\n",
    "        self.HitAnomaly = HitAnomalyLayerConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                transformer_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self._global.load(transformer_config)\n",
    "        self.BERT.load(transformer_config)\n",
    "        self.HitAnomaly.load(transformer_config)\n",
    "\n",
    "def set_attributes_from_object(self, *args):\n",
    "    try:\n",
    "        for obj in args:\n",
    "            for attr_key, attr in obj.__dict__.items():\n",
    "                setattr(self, attr_key, attr)\n",
    "    except Exception as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model: int, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEncodingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1  # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dff: int,\n",
    "        rate=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.multi_headed_attention = MultiHeadAttention(num_heads=num_heads,\n",
    "                                                         key_dim=d_model // num_heads,\n",
    "                                                         dropout=0.1,\n",
    "                                                         attention_axes=(1))\n",
    "\n",
    "        self.feed_forward_network = Sequential([\n",
    "            Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "            Dense(d_model, activation='relu')  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        # (1) - Attention Score\n",
    "#         logger.info('MULTIHEADED ATTENTION')\n",
    "#         logger.info(x.shape)\n",
    "        attn_output, attn_weights = self.multi_headed_attention(\n",
    "            x,\n",
    "            x,\n",
    "            return_attention_scores=True)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (2) - Add & Normalize\n",
    "        attn_output = self.dropout1(attn_output, training=True)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (3) - Feed Forward NN\n",
    "        feed_forward_output = self.feed_forward_network(out1)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (4) - Add & Normalize\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=True)\n",
    "        out2 = self.layernorm2(out1 + feed_forward_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return tf.convert_to_tensor(out2), tf.convert_to_tensor(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTLayer(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_config: TransformerGlobalConfig,\n",
    "        config: BERTLayerConfig):\n",
    "        super(BERTLayer, self).__init__()\n",
    "\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            global_config,\n",
    "            config)\n",
    "\n",
    "        self.bert_layer_blocks = [EncoderBlock(\n",
    "            self.d_model,\n",
    "            self.num_attention_heads,\n",
    "            self.dff,\n",
    "            rate=self.dropout_rate) for _ in range(self.num_encoder_layers)]\n",
    "\n",
    "    def call(self, input_: tf.tuple, **kwargs):\n",
    "        enc_input = input_[0]\n",
    "#         logger.info('BERT LAYER')\n",
    "#         logger.info(enc_input.shape)\n",
    "        encoding_padding_mask = None\n",
    "        # BERT for Log Sequence Embedding\n",
    "        for layer_idx in range(self.num_encoder_layers):\n",
    "#             logger.info('BERT LAYER LOOP')\n",
    "#             logger.info(enc_input.shape)\n",
    "            enc_output, attention = self.bert_layer_blocks[layer_idx](enc_input, encoding_padding_mask)\n",
    "            bert_ret = tf.tuple(enc_output, attention)\n",
    "        return bert_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HitAnomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HitAnomalyLayer(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        global_config: TransformerGlobalConfig,\n",
    "        config: HitAnomalyLayerConfig):\n",
    "        super(HitAnomalyLayer, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            global_config,\n",
    "            config)\n",
    "\n",
    "        self.encoding_blocks = [EncoderBlock(\n",
    "            self.d_model,\n",
    "            self.num_attention_heads,\n",
    "            self.dff,\n",
    "            rate=self.dropout_rate\n",
    "        ) for _ in range(self.num_encoder_layers)]\n",
    "\n",
    "        self.hidden_layer_output = []\n",
    "\n",
    "#     @tf.function(jit_compile=True)\n",
    "    def call(self, input_: tf.tuple, **kwargs):\n",
    "        enc_input = input_[0]\n",
    "        encoding_padding_mask = None\n",
    "\n",
    "        # Encoder Block Hidden Layers for Log Encoder\n",
    "        # (batch_size, inp_seq_len, d_model), (batch_size, class, inp_seq_len, inp_seq_len)\n",
    "        for layer_idx in range(self.num_encoder_layers - 1):\n",
    "            enc_output, att = self.encoding_blocks[layer_idx](enc_input, encoding_padding_mask)\n",
    "            self.hidden_layer_output.append(enc_output)\n",
    "\n",
    "        fin_output = enc_output\n",
    "        final_output = tf.reduce_mean(fin_output, axis=1)\n",
    "        final_output = tf.expand_dims(final_output, axis=0)\n",
    "\n",
    "        # Last Encoding Block for Log Sequence Representation\n",
    "        out, att = self.encoding_blocks[self.num_encoder_layers - 1](final_output, encoding_padding_mask)\n",
    "        self.hidden_layer_output.append(out)\n",
    "\n",
    "        # Final Pooling Layer\n",
    "        seq_representation = tf.reduce_mean(out, axis=1)\n",
    "\n",
    "        return seq_representation, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PrimeTokenizer,\n",
    "        config: TransformerConfig):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        set_attributes_from_object(\n",
    "            self,\n",
    "            config._global)\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            self.vocab_size,\n",
    "            self.d_model,\n",
    "            input_length=self.max_seq_len)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            self.max_seq_len,\n",
    "            self.d_model)\n",
    "\n",
    "        self.bert_layer = BERTLayer(\n",
    "            config._global,\n",
    "            config.BERT)\n",
    "\n",
    "        self.hitanomaly_layer = HitAnomalyLayer(\n",
    "            self.vocab_size,\n",
    "            config._global,\n",
    "            config.HitAnomaly)\n",
    "\n",
    "        #self.dropout = Dropout(rate)\n",
    "\n",
    "#     @tf.function(jit_compile=True)\n",
    "    def call(self, input_tuple: tf.tuple, **kwargs):\n",
    "        log_batch = input_tuple[0]\n",
    "#         logger.info('INITIAL')\n",
    "#         logger.info(log_batch.shape)\n",
    "        encoding_padding_mask = None # input_tuple[1]\n",
    "\n",
    "        embedding_tensor = self.embedding(log_batch) # (batch_size, input_seq_len, d_model)\n",
    "#         logger.info('POST EMBEDDING LAYER')\n",
    "#         logger.info(embedding_tensor.shape)\n",
    "\n",
    "        embedding_tensor = self.pos_encoding(embedding_tensor)\n",
    "#         logger.info('POST POSITIONAL ENCODING')\n",
    "#         logger.info(embedding_tensor.shape)\n",
    "        #embedding_tensor = self.dropout(embedding_tensor, training=TRAINING)\n",
    "\n",
    "        # BERT for Log Sequence Embedding\n",
    "        bert_arg = tf.tuple(embedding_tensor, encoding_padding_mask)\n",
    "        bert_ret = self.bert_layer(bert_arg)\n",
    "\n",
    "        # Encoder Block Hidden Layers for Log Sequence Representation\n",
    "#         seq_representation, att = self.hitanomaly_layer(tf.tuple(enc_output, encoding_padding_mask))\n",
    "\n",
    "\n",
    "        return bert_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# -- Transformer Model -- #\n",
    "transformer_config_path = SOURCE + '/assets/notebooks/TransformerConfig.yaml'\n",
    "transformer_config = TransformerConfig()\n",
    "transformer_config.load(transformer_config_path)\n",
    "# optimus_prime = Transformer(prime_tokenizer, transformer_config)\n",
    "\n",
    "t_config = transformer_config._global\n",
    "\n",
    "# -- Pipeline Info -- #\n",
    "attns = []\n",
    "\n",
    "# -- Data Batches -- #\n",
    "# batched_dataset = process_all_batches(n_iter, log_labels, t_config.batch_size)\n",
    "\n",
    "# -- Model Metrics -- #\n",
    "learning_rate = CustomSchedule(t_config.d_model)\n",
    "epoch_loss = Mean(name='train_loss')\n",
    "epoch_accuracy = Mean(name='train_accuracy')\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# -- Classification Step Layers -- #\n",
    "add_att_layer = AdditiveAttention()\n",
    "softmax = Softmax()\n",
    "s1 = Sequential([\n",
    "    Dense(t_config.batch_size, activation=t_config.activation),\n",
    "    Dense(4, activation=t_config.activation),\n",
    "    Softmax()\n",
    "])\n",
    "\n",
    "# -- Checkpoints -- #\n",
    "# checkpoint_path = SOURCE + \"checkpoints/\"\n",
    "# checkpoint = Checkpoint(step=tf.Variable(1), transformer=optimus_prime, optimizer=optimizer)\n",
    "# checkpoint_manager = CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "writer = tf.summary.create_file_writer(SOURCE + t_config.logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = transformer_config._global.batch_size\n",
    "max_seq_len = transformer_config._global.max_seq_len\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=([batch_size, max_seq_len]), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=([batch_size]), dtype=tf.int8)\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature,\n",
    "             experimental_compile=True)\n",
    "def train_step(log_batch: tf.Tensor, labels: tf.Tensor):\n",
    "\n",
    "    transformer_input = tf.tuple([\n",
    "        log_batch,  # <tf.Tensor: shape=(batch_size, max_seq_len), dtype=int32>\n",
    "        labels  # <tf.Tensor: shape=(batch_size, num_classes), dtype=float32>\n",
    "    ])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        transformer_ret = optimus_prime(transformer_input)\n",
    "#         a_s = add_att_layer([Rs, Rs])\n",
    "#         y = softmax(a_s * Rs)\n",
    "#         print(a_s.shape)\n",
    "        # y = Rs\n",
    "#         loss = tf.py_function(loss_function, [labels, y], tf.float32)\n",
    "#         pred = s1(y)\n",
    "#         labels = tf.cast(labels, tf.int8)\n",
    "    # Optimize the model\n",
    "#     grads = tape.gradient(loss, optimus_prime.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, optimus_prime.trainable_variables))\n",
    "\n",
    "#     acc = accuracy_function(labels, pred)\n",
    "\n",
    "    # Tracking Progress\n",
    "#     epoch_loss.update_state(loss)  # Adding Batch Loss\n",
    "#     epoch_accuracy.update_state(acc)\n",
    "\n",
    "    return transformer_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_step_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attentions = []\n",
    "\n",
    "for epoch in range(t_config.epoch):\n",
    "\n",
    "    start = time.time()\n",
    "    epoch_loss.reset_states()\n",
    "    epoch_accuracy.reset_states()\n",
    "    dataset_iter = iter(batched_dataset)\n",
    "\n",
    "    t = tqdm(range(n_iter), desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%}\".format(0, 0, 0), position=0, leave=True)\n",
    "    for _ in t:\n",
    "        batch = next(dataset_iter)\n",
    "        log_batch = batch[0]\n",
    "        labels = batch[1]\n",
    "\n",
    "        # Returns Eager Tensor for Predictions\n",
    "#         tf.summary.trace_on()\n",
    "#         tf.profiler.experimental.start(SOURCE + t_config.logdir)\n",
    "\n",
    "#         with writer.as_default():\n",
    "        transforer_ret = train_step(log_batch, labels)\n",
    "        attentions.append(transformer_ret)\n",
    "          # with tf.summary.record_if(True):\n",
    "\n",
    "#             tf.summary.trace_export(\n",
    "#               name = \"training_trace\",\n",
    "#               step=0,\n",
    "#               profiler_outdir=SOURCE + t_config.logdir\n",
    "#             )\n",
    "\n",
    "#         tf.profiler.experimental.stop()\n",
    "#         tf.summary.trace_off()\n",
    "\n",
    "#         checkpoint.step.assign_add(1)\n",
    "\n",
    "#         if int(checkpoint.step) % 10 == 0:\n",
    "#             save_path = checkpoint_manager.save()\n",
    "\n",
    "        t.set_description(desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%} \".format(epoch,\n",
    "                                                                    epoch_loss.result(),\n",
    "                                                                    epoch_accuracy.result()))\n",
    "        t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(acc, SOURCE + '/results/attention.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(Rs, SOURCE + '/results/Rs.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
