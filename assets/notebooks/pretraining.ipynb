{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/assets\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-14 20:46:28.688195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Base -- #\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import yaml\n",
    "import csv\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "from yaspin import yaspin\n",
    "\n",
    "# -- Tokenizer -- #\n",
    "import tokenizers\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# -- PreTrained BERT -- #\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# -- Metrics -- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- Misc Models -- #\n",
    "import drain3\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -- Custom -- #\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertForPreTraining\n",
    "from libs.transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from libs.transformers.src.transformers.modeling_tf_utils import shape_list\n",
    "from libs.transformers.src.transformers.data.data_tf_collator import TFDataCollatorForLanguageModeling\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sqlite_to_csv(inputFolder, ext, tableName):\n",
    "    \"\"\" inputFolder - Folder where sqlite files are located. \n",
    "        ext - Extension of your sqlite file (eg. db, sqlite, sqlite3 etc.)\n",
    "        tableName - table name from which you want to select the data.\n",
    "    \"\"\"\n",
    "    csvWriter = csv.writer(open(inputFolder+'/output.csv', 'w', newline=''))\n",
    "    for file1 in os.listdir(inputFolder):\n",
    "        if file1.endswith('.'+ext):\n",
    "            conn = sql.connect(inputFolder+'/'+file1)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM \"+tableName)\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                csvWriter.writerow(row)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-14 20:46:29.744782: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-14 20:46:29.745314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-14 20:46:29.784943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-14 20:46:29.785366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2021-07-14 20:46:29.785383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-14 20:46:29.787035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-14 20:46:29.787066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-14 20:46:29.787728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-14 20:46:29.787868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-14 20:46:29.789531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-14 20:46:29.789899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-14 20:46:29.789985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-14 20:46:29.790091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-14 20:46:29.790546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-14 20:46:29.790920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SOURCE = '/home/' + os.environ['USER']\n",
    "CONTAINER = 'core.soaesb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s | %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def database_builder(path: str) -> pd.DataFrame():\n",
    "    logger.info('Building DataFrame ...')\n",
    "    (_, _, files) = next(os.walk(path))\n",
    "    sql_query = 'SELECT * FROM logs'\n",
    "    data = []\n",
    "    for f in files:\n",
    "        if '.db' in f:\n",
    "            conn = create_connection(path + f)\n",
    "            d = pd.read_sql_query(sql_query, conn)\n",
    "            data.append(d)\n",
    "    logger.info('...complete!')\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def create_connection(path: str) -> sql.Connection:\n",
    "    \"\"\"\n",
    "    Creates a database connection\n",
    "    :param path: str\n",
    "        path to database object\n",
    "    :return sql.Connection\n",
    "        a connection to the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sql.connect(path)\n",
    "        logger.info('Connected to database ' + path)\n",
    "        return conn\n",
    "    except sql.Error as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-14 20:46:29,808 INFO | Building DataFrame ...\n",
      "2021-07-14 20:46:29,808 INFO | Connected to database /home/jovyan/data/elastic_logs.db\n",
      "2021-07-14 20:46:30,452 INFO | ...complete!\n"
     ]
    }
   ],
   "source": [
    "dataset = database_builder(SOURCE + '/data/')\n",
    "container_dataset = dataset[dataset['container_name'] == CONTAINER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_attributes(self, config: dict):\n",
    "    try:\n",
    "        config = config[self.__class__.__name__]\n",
    "    except Exception as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning('No configuration found for ' +\n",
    "                       self.__class__.__name__)\n",
    "\n",
    "    for attr in config.keys():\n",
    "        setattr(self, attr, config[attr])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingGlobalConfig:\n",
    "    embed_size: int = 512\n",
    "    max_vocab_size: int = 2000\n",
    "    buffer_size: int = 10000\n",
    "    global_training: bool = True\n",
    "    path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhraserModelConfig:\n",
    "    min_count: int = 5\n",
    "    threshold: float = 7\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'phrase_model.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClusteringConfig:\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'template_miner.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class PreprocessingPipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.PreprocessingGlobalConfig = PreprocessingGlobalConfig()\n",
    "        self.PhraserModelConfig = PhraserModelConfig()\n",
    "        self.TextClusteringConfig = TextClusteringConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                preprocessing_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self.PreprocessingGlobalConfig.load(preprocessing_config)\n",
    "        self.PhraserModelConfig.load(preprocessing_config)\n",
    "        self.TextClusteringConfig.load(preprocessing_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PrimeTokenizer:\n",
    "    def __init__(self, max_seq_length: int):\n",
    "        self.prime_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.prime_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "        self.prime_tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.prime_tokenizer.decoder = decoders.WordPiece()\n",
    "        self.prime_tokenizer.enable_padding(length=max_seq_length)\n",
    "        self.prime_tokenizer.enable_truncation(max_seq_length)\n",
    "\n",
    "        self.prime_tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.trainer = WordPieceTrainer(\n",
    "            vocab_size=153411,\n",
    "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "    def text_to_sequence(self, input_) -> List[tokenizers.Encoding]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.encode_batch(input_)\n",
    "        return self.prime_tokenizer.encode(input_)\n",
    "\n",
    "    def sequence_to_text(self, input_) -> List[str]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.decode_batch(batch)\n",
    "        return self.prime_tokenizer.decode(input_)\n",
    "\n",
    "    def train(self, data):\n",
    "        log_itr = iter(data)\n",
    "        self.prime_tokenizer.train_from_iterator(log_itr, self.trainer)\n",
    "        self.save()\n",
    "\n",
    "    def get_tokenizer(self) -> Tokenizer:\n",
    "        return self.prime_tokenizer\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.prime_tokenizer.get_vocab()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.prime_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def save(self):\n",
    "        self.prime_tokenizer.save(SOURCE + \"/results/prime_tokenizer.json\")\n",
    "        \n",
    "    def load(self):\n",
    "        self.prime_tokenizer = Tokenizer.from_file(SOURCE + \"/results/prime_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "#     if not os.path.exists(path):\n",
    "#         return\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "#     elif os.path.isdir(path):\n",
    "#         shutil.rmtree(path)\n",
    "#         return\n",
    "\n",
    "    joblib.dump(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhraseCaptureLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PhraserModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: PhraserModelConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(PhraserModel, self).__init__()\n",
    "        self.min_count = config.min_count\n",
    "        self.threshold = config.threshold\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model:\n",
    "            self.phrase_model = joblib.load(SOURCE +\n",
    "                                            self.path +\n",
    "                                            self.model_name)\n",
    "        else:\n",
    "            self.phrase_model = Phrases(min_count=self.min_count,\n",
    "                                        threshold=self.threshold)\n",
    "\n",
    "    def __call__(self, corpus: pd.DataFrame, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        def reorganize_return(corpus_with_phrases):\n",
    "            log_list = []\n",
    "            for tokenized_log in corpus_with_phrases:\n",
    "                log_list.append(' '.join(tokenized_log))\n",
    "            return log_list\n",
    "\n",
    "        split_corpus = [log.split(' ') for log in corpus['log']]\n",
    "\n",
    "        corpus_with_phrases = None\n",
    "        if not training:\n",
    "            frozen_model = self.phrase_model.freeze()\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "        else:\n",
    "            self.phrase_model.add_vocab(split_corpus)\n",
    "\n",
    "            if self.save_model:\n",
    "                save_model(self.phrase_model, SOURCE + self.path + self.model_name)\n",
    "\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "            \n",
    "        return reorganize_return(corpus_with_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextClusteringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextClustering:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: TextClusteringConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(TextClustering, self).__init__()\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model is True:\n",
    "            self.template_miner = joblib.load(SOURCE +\n",
    "                                              self.path +\n",
    "                                              self.model_name)\n",
    "        else:\n",
    "            self.template_miner = drain3.TemplateMiner()\n",
    "\n",
    "    def __call__(self, corpus: list, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        if training:\n",
    "            for log in corpus:\n",
    "                self.template_miner.add_log_message(log)\n",
    "            if self.save_model:\n",
    "                save_model(self.template_miner,\n",
    "                           SOURCE + self.path + self.model_name)\n",
    "\n",
    "            for idx, log in enumerate(corpus):\n",
    "                template = self.template_miner.match(log).get_template()\n",
    "                corpus[idx] = template\n",
    "\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster) for cluster in corpus]\n",
    "        else:\n",
    "#             log_list = self.get_unique_templates()\n",
    "#             print(f'Length of the log list: {len(log_list)}')\n",
    "#             return log_list\n",
    "            log_list = []\n",
    "            log_set = set()\n",
    "            for log in corpus:\n",
    "                match_cluster = self.template_miner.match(log)\n",
    "                if match_cluster is None:\n",
    "                    match_cluster = self.template_miner.add_log_message(log)['template_mined']\n",
    "                    log_set.add(match_cluster)\n",
    "                else:\n",
    "                    log_set.add(match_cluster.get_template())\n",
    "        \n",
    "#             l = [re.sub(pattern=r' +',\n",
    "#                            repl=' ',\n",
    "#                            string=cluster) for cluster in log_list]\n",
    "            return list(log_set)\n",
    "        \n",
    "    def get_unique_templates(self) -> list:\n",
    "        template_list = []\n",
    "        for cluster in self.template_miner.drain.clusters:\n",
    "            template_list.append(cluster.get_template())\n",
    "        return [re.sub(pattern=r' +',\n",
    "                       repl=' ',\n",
    "                       string=cluster) for cluster in template_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_all_batches(n_iter, log_labels, batch_size):\n",
    "    batches = []\n",
    "\n",
    "    for idx in range(n_iter + 1):\n",
    "        log_batch, labels = process_batch(dataset, idx, log_labels, batch_size)\n",
    "\n",
    "        batches.append((log_batch, labels))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def process_batch(dataset: pd.DataFrame,\n",
    "                  idx: int,\n",
    "                  labels: dict,\n",
    "                  batch_size: int) -> tuple:\n",
    "    start_window = idx * batch_size\n",
    "    end_window = (idx + 1) * batch_size\n",
    "    batched_data = dataset.iloc[start_window:end_window]\n",
    "    encoded_batch = prime_tokenizer.text_to_sequence(batched_data['log'].to_list())\n",
    "    id_batch = [log.ids for log in encoded_batch]\n",
    "#     y_batch = labels[batched_data['label']]\n",
    "    y_batch = [labels[idx] for idx in batched_data['label']]\n",
    "\n",
    "    tf_idf = tf.convert_to_tensor(id_batch, dtype=tf.float32)\n",
    "    y_idf  = tf.convert_to_tensor(y_batch, dtype=tf.float32)\n",
    "    \n",
    "    return tf_idf, y_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EncodedSeq = List[int]\n",
    "\n",
    "def normalize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    # remove timestamps and double spaces\n",
    "    regexp = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:               # Match all enclosed\n",
    "        \\d{4}-\\d{2}-\\d{2} # YYYY-MM-DD\n",
    "        [\\sT]             # Accept either a space or T\n",
    "        \\d{2}:\\d{2}:\\d{2} # HH:MM:SS\n",
    "        ([.,]\\d{3}|\\s)    # Accept either a space or milliseconds\n",
    "        )                 # End timestamp match\n",
    "        | (?:\\s{2,})      # Remove double spaces   \n",
    "        | [^a-zA-Z\\d:]    # Clean non-alphanumeric characters\n",
    "        \"\"\", re.X)        # re.X enables comments and whitespace\n",
    "\n",
    "    c_logs = deepcopy(logs)\n",
    "    c_logs.loc[:, 'log'].replace(\n",
    "        to_replace=regexp, \n",
    "        value=' ', \n",
    "        regex=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return c_logs\n",
    "\n",
    "\n",
    "def extract_unique_labels(logs: pd.DataFrame) -> dict:\n",
    "    # -- Labels -- #\n",
    "    label_unique = logs['label'].unique()\n",
    "    binary_labels = LabelEncoder().fit_transform(label_unique)\n",
    "\n",
    "    log_labels = {}\n",
    "    # TODO: This seems a bit messy, could it be cleaned up? \n",
    "    for idx, label in enumerate(label_unique):\n",
    "        log_labels.update({\n",
    "            label: binary_labels[idx]\n",
    "        })\n",
    "    return log_labels\n",
    "\n",
    "\n",
    "def create_sentence_pairing(\n",
    "    logs: pd.DataFrame, \n",
    "    n_iter: int, \n",
    "    batch_size: int\n",
    ") -> Tuple[List[EncodedSeq], List[EncodedSeq], List[int]]:\n",
    "\n",
    "    first_seqs, second_seqs, nsp_labels = [], [], []\n",
    "    log_list = logs['log'].to_list()\n",
    "    \n",
    "    for idx in range(len(log_list)):\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "            \n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            # TODO: can rand_idx == idx? or should we add a check here?\n",
    "            second_value = log_list[rand_idx]\n",
    "            \n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "            \n",
    "        first_seqs.append(log_list[idx])\n",
    "        second_seqs.append(second_value)\n",
    "        \n",
    "    nsp_labels = tf.reshape(\n",
    "        tf.constant(nsp_labels), \n",
    "        (n_iter, batch_size),\n",
    "        'nsp_labels'\n",
    "    )\n",
    "        \n",
    "    return first_seqs, second_seqs, nsp_labels\n",
    "\n",
    "\n",
    "def collate_data(encodings, tokenizer, max_seq_len, batch_size):\n",
    "    # Assure data is non-ragged and are of type tf.Tensor\n",
    "    encodings['input_ids'] = tf.ragged.constant(encodings['input_ids']).to_tensor(0)\n",
    "    encodings['token_type_ids'] = tf.ragged.constant(encodings['token_type_ids']).to_tensor(0)\n",
    "    encodings['attention_mask'] = tf.ragged.constant(encodings['attention_mask']).to_tensor(0)\n",
    "\n",
    "    batch_encodings = tf.data.Dataset.from_tensor_slices(encodings.data)\n",
    "\n",
    "    data_collator = TFDataCollatorForLanguageModeling(\n",
    "        tokenizer,\n",
    "        padding_length=max_seq_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return data_collator(batch_encodings, drop_remainder=True)\n",
    "\n",
    "\n",
    "class UnsupervisedLearningPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PreprocessingPipelineConfig, \n",
    "        epochs: int = 3, \n",
    "        batch_size: int = 50, \n",
    "        seq_length: int = 200\n",
    "    ) -> None:\n",
    "\n",
    "        self.normalized_logs = None\n",
    "        self.log_labels = None\n",
    "        self.bert_tokenizer = PrimeTokenizer(seq_length)\n",
    "        self.fast_tokenizer = None\n",
    "        self.train_dataset = None\n",
    "        self.token_logs = None\n",
    "        self.bert_config = None\n",
    "        self.BERT = None\n",
    "        self.max_seq_len = seq_length\n",
    "        self.n_logs = 0\n",
    "        self.n_iter = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.logs_with_phrases = list()\n",
    "        self.logs_as_templates = None\n",
    "        self.data_collator = None\n",
    "        \n",
    "        self.pm = PhraserModel(config.PhraserModelConfig, \n",
    "                               config.PreprocessingGlobalConfig)\n",
    "        \n",
    "        self.tc = TextClustering(config.TextClusteringConfig,\n",
    "                                 config.PreprocessingGlobalConfig)\n",
    "        \n",
    "\n",
    "    def initialize_fast_tokenizer(self):\n",
    "        tokenizer_obj = self.bert_tokenizer.get_tokenizer()\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj)\n",
    "        fast_tokenizer.model_max_length = self.max_seq_len\n",
    "        fast_tokenizer.unk_token = \"[UNK]\"\n",
    "        fast_tokenizer.sep_token = \"[SEP]\"\n",
    "        fast_tokenizer.pad_token = \"[PAD]\"\n",
    "        fast_tokenizer.cls_token = \"[CLS]\"\n",
    "        fast_tokenizer.mask_token = \"[MASK]\"\n",
    "        self.fast_tokenizer = fast_tokenizer\n",
    "        return fast_tokenizer\n",
    "\n",
    "\n",
    "    def get_pre_training_data(self):\n",
    "        labels = self.normalized_logs['label'].to_list()\n",
    "        log_labels = [self.log_labels[l] for l in labels]\n",
    "\n",
    "        fast_tokenizer = self.initialize_fast_tokenizer()\n",
    "        \n",
    "        first_sequences, pair_sequences, nsp_labels = create_sentence_pairing(self.normalized_logs, self.n_iter, self.batch_size)\n",
    "        \n",
    "        encodings = fast_tokenizer(\n",
    "            first_sequences,\n",
    "            pair_sequences,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "                        \n",
    "        encoded_objs = collate_data(\n",
    "            encodings,\n",
    "            fast_tokenizer,\n",
    "            self.max_seq_len,\n",
    "            self.batch_size\n",
    "        )\n",
    "        \n",
    "        pre_training_data = dict()\n",
    "        target_values = dict()\n",
    "        for key in list(encoded_objs)[0].keys():\n",
    "            temp_tensor = [tf.constant(x[key].numpy()) for x in encoded_objs]\n",
    "            if key == \"labels\":\n",
    "                target_values[key] = temp_tensor\n",
    "                continue\n",
    "            pre_training_data[key] = temp_tensor\n",
    "            \n",
    "        target_values[\"next_sentence_label\"] = nsp_labels\n",
    "        \n",
    "        pre_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            pre_training_data,\n",
    "            target_values\n",
    "        ))\n",
    "        \n",
    "        return pre_train_dataset\n",
    "\n",
    "    \n",
    "    def train_bert_tokenizer(self, load_model=False):\n",
    "        if load_model:\n",
    "            self.bert_tokenizer.load()\n",
    "        else:\n",
    "            self.bert_tokenizer.train(self.logs_as_templates)\n",
    "\n",
    "\n",
    "    def pre_train_bert(self):\n",
    "        self.bert_config = BertConfig(\n",
    "            vocab_size=self.bert_tokenizer.get_vocab_size(),\n",
    "            hidden_size=512,\n",
    "            num_hidden_layers=8,\n",
    "            num_attention_heads=8\n",
    "        )\n",
    "        self.BERT = TFBertForPreTraining(self.bert_config)\n",
    "        self.BERT.resize_token_embeddings(len(self.fast_tokenizer))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Prepare the metrics.\n",
    "        train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        outputs = None\n",
    "        with yaspin(text=\"\", color='blue') as sp:\n",
    "    \n",
    "            # Training BERT\n",
    "    \n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Finished Training Epochs')\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def run_bert(self, batch):\n",
    "        return self.BERT(batch,\n",
    "                         output_attentions=True)\n",
    "\n",
    "    def fit(self, logs: pd.DataFrame):\n",
    "        assert len(logs.index) > 0, 'process received an empty dataframe!'\n",
    "        \n",
    "        with yaspin(text=\"Normalizing Logs\", color='green') as sp:\n",
    "            self.normalized_logs = normalize_logs(logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log normalization')\n",
    "            \n",
    "            sp.text = \"Extracting phrases\"\n",
    "            self.logs_with_phrases = self.pm(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed phrase extraction')\n",
    "            \n",
    "            sp.text = \"Converting to log templates\"\n",
    "            self.logs_as_templates = np.array(self.tc(self.logs_with_phrases))\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log template conversion')\n",
    "        \n",
    "            sp.text = \"Extracting Unique Labels\"\n",
    "            self.log_labels = extract_unique_labels(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed extracting unique labels')\n",
    "\n",
    "            sp.text = \"Training Tokenizer\"\n",
    "            self.train_bert_tokenizer()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed training of custom tokenizer')\n",
    "            \n",
    "            self.n_logs = len(self.normalized_logs['log'])\n",
    "            self.n_iter = self.n_logs // self.batch_size\n",
    "                        \n",
    "            sp.text = \"Processing training dataset\"\n",
    "            self.train_dataset = self.get_pre_training_data()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed processing training dataset')\n",
    "            \n",
    "            sp.text = \"Pretraining BERT\"\n",
    "            outputs = self.pre_train_bert()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed BERT pretraining')\n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    def transform(self, batch: pd.DataFrame):\n",
    "        x = self.normalize_logs(batch)\n",
    "        x = self.pm(x, False)\n",
    "        x = list(self.tc(x, False))\n",
    "        x = self.serve_batch(x)\n",
    "        return self.run_bert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V Pipeline Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_path = SOURCE + '/assets/notebooks/PreprocessingConfig.yaml'\n",
    "preprocessing_config = PreprocessingPipelineConfig()\n",
    "preprocessing_config.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # -- Unsupervised Learning Pipeline -- #\n",
    "\n",
    "# '''\n",
    "# Input: pd.DataFrame with batch_size number of rows \n",
    "# Seq: \n",
    "#     Normalize \n",
    "#     Phraser\n",
    "#     Clustering\n",
    "#     Extract Unique Layers\n",
    "#     BERT\n",
    "# Returns: transformers.TFBertForPreTrainingOutput\n",
    "# '''\n",
    "\n",
    "# # --- SUBWORD TOKENIZER --\n",
    "# w2vp = UnsupervisedLearningPipeline(preprocessing_config, epochs=5, batch_size=10, seq_length=100)\n",
    "# training_outputs = w2vp.fit(dataset[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm_logs = normalize_logs(dataset[:1000])\n",
    "dt = norm_logs.drop([\"label\", \"container_name\", \"timestamp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batchsize_for_testing = 2\n",
    "max_seq_length = 300\n",
    "\n",
    "data = Dataset.from_pandas(dt[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = w2vp.bert_tokenizer.get_tokenizer()\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj, truncation=True)\n",
    "fast_tokenizer.model_max_length = max_seq_length\n",
    "fast_tokenizer.unk_token = \"[UNK]\"\n",
    "fast_tokenizer.sep_token = \"[SEP]\"\n",
    "fast_tokenizer.pad_token = \"[PAD]\"\n",
    "fast_tokenizer.cls_token = \"[CLS]\"\n",
    "fast_tokenizer.mask_token = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_generator(dataset, tokenizer, mlm_probability=0.15, pad_to_multiple_of=max_seq_length):\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError(\"This tokenizer does not have a mask token which is necessary for masked language modeling. \")\n",
    "    # Trim off the last partial batch if present\n",
    "    sample_ordering = np.random.permutation(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        special_tokens_mask = example.pop(\"special_tokens_mask\", None)\n",
    "        example[\"input_ids\"], example[\"labels\"] = mask_tokens(\n",
    "            example[\"input_ids\"], mlm_probability, tokenizer, special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            example[\"labels\"][example[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "        example = {key: tf.convert_to_tensor(arr) for key, arr in example.items()}\n",
    "\n",
    "        yield example, {\"labels\": example[\"labels\"], \"next_sentence_label\": example[\"next_sentence_label\"]}  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "\n",
    "def mask_tokens(inputs, mlm_probability, tokenizer, special_tokens_mask):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "    labels = np.copy(inputs)\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "    probability_matrix = np.random.random_sample(labels.shape)\n",
    "    special_tokens_mask = special_tokens_mask.astype(np.bool_)\n",
    "\n",
    "    probability_matrix[special_tokens_mask] = 0.0\n",
    "    masked_indices = probability_matrix > (1 - mlm_probability)\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = (np.random.random_sample(labels.shape) < 0.8) & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = (np.random.random_sample(labels.shape) < 0.5) & masked_indices & ~indices_replaced\n",
    "    random_words = np.random.randint(low=0, high=len(tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64)\n",
    "    inputs[indices_random] = random_words\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73f5ca0a0774f5a8b241de0ea985268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset line_by_line:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd97fc4eb544c5fbe6d758f6aeb780b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset line_by_line:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_sentence_pairing(examples):\n",
    "    first_seqs = []\n",
    "    nsp_labels = []\n",
    "\n",
    "    examples[\"log\"] = [\n",
    "        line for line in examples[\"log\"] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    \n",
    "    log_list = list(examples['log'])\n",
    "    for idx in range(len(log_list)):\n",
    "        first_value = log_list[idx]\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "\n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            second_value = log_list[rand_idx]\n",
    "\n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "\n",
    "        first_seqs.append((first_value, second_value))\n",
    "\n",
    "#     nsp_labels = tf.reshape(tf.constant(nsp_labels), (self.n_iter, self.batch_size))\n",
    "\n",
    "    return {\"log\": first_seqs, \"next_sentence_label\": nsp_labels}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    return fast_tokenizer(\n",
    "        examples[\"log\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = data.map(\n",
    "    create_sentence_pairing,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"log\"],\n",
    "    desc=\"Running tokenizer on dataset line_by_line\"\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"log\"],\n",
    "    desc=\"Running tokenizer on dataset line_by_line\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(tokenized_datasets))), test_size=.30\n",
    ")\n",
    "\n",
    "eval_dataset = tokenized_datasets.select(val_indices)\n",
    "train_dataset = tokenized_datasets.select(train_indices)\n",
    "\n",
    "train_signature = {\n",
    "    feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "    for feature in train_dataset.features\n",
    "    if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "}\n",
    "train_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "train_signature = (train_signature, {\"labels\": train_signature[\"labels\"], \"next_sentence_label\": train_signature[\"next_sentence_label\"]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TFDataCollatorForLanguageModeling(\n",
    "    fast_tokenizer,\n",
    "    padding_length=max_seq_length,\n",
    "    batch_size=batchsize_for_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_generator = partial(data_collator, train_dataset, fast_tokenizer)\n",
    "\n",
    "tf_train_dataset = (\n",
    "    tf.data.Dataset.from_generator(tokenized_generator, output_signature=train_signature)\n",
    "    .batch(batch_size=batchsize_for_testing, drop_remainder=True)\n",
    "    .repeat(int(5))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_generator = partial(data_collator, eval_dataset, fast_tokenizer)\n",
    "eval_signature = {\n",
    "    feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "    for feature in eval_dataset.features\n",
    "    if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "}\n",
    "eval_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "eval_signature[\"labels\"] = eval_signature[\"input_ids\"]\n",
    "eval_signature = (eval_signature, {\"labels\": eval_signature[\"labels\"], \"next_sentence_label\": eval_signature[\"next_sentence_label\"]})\n",
    "tf_eval_dataset = (\n",
    "    tf.data.Dataset.from_generator(eval_generator, output_signature=eval_signature)\n",
    "    .batch(batch_size=batchsize_for_testing, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batches_per_epoch = len(train_dataset) // batchsize_for_testing\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=1e-4,\n",
    "    num_train_steps=int(5 * batches_per_epoch),\n",
    "    num_warmup_steps=2,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=0.1,\n",
    "    weight_decay_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-14 20:49:09.040904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-14 20:49:09.316973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    }
   ],
   "source": [
    "bert_config = BertConfig(\n",
    "    vocab_size=w2vp.bert_tokenizer.get_vocab_size(),\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8\n",
    ")\n",
    "\n",
    "phobert = TFBertForPreTraining(bert_config)\n",
    "phobert.resize_token_embeddings(len(w2vp.fast_tokenizer))\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    # make sure only labels that are not equal to -100\n",
    "    # are taken into account as loss\n",
    "    masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n",
    "    masked_lm_reduced_logits = tf.boolean_mask(\n",
    "        tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n",
    "        mask=masked_lm_active_loss,\n",
    "    )\n",
    "    masked_lm_labels = tf.boolean_mask(\n",
    "        tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n",
    "    )\n",
    "    next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "    next_sentence_reduced_logits = tf.boolean_mask(\n",
    "        tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "    )\n",
    "    next_sentence_label = tf.boolean_mask(\n",
    "        tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "    )\n",
    "    \n",
    "    return (masked_lm_labels, masked_lm_reduced_logits), (next_sentence_label, next_sentence_reduced_logits)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = phobert(x, training=True)\n",
    "        mlm, nsp = compute_loss(y, (logits[\"prediction_logits\"], logits[\"seq_relationship_logits\"]))\n",
    "        masked_lm_loss = loss_fn(mlm[0], mlm[1])\n",
    "        next_sentence_loss = loss_fn(nsp[0], nsp[1])\n",
    "        loss_value = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "    grads = tape.gradient(loss_value, phobert.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, phobert.trainable_weights))\n",
    "    \n",
    "\n",
    "    # Update training metric.\n",
    "    train_acc_metric.update_state(mlm[0], mlm[1])\n",
    "    train_acc_metric.update_state(nsp[0], nsp[1])\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = phobert(x, training=False)\n",
    "    mlm, nsp = compute_loss(y, (val_logits[\"prediction_logits\"], val_logits[\"seq_relationship_logits\"]))\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(mlm[0], mlm[1])\n",
    "    val_acc_metric.update_state(nsp[0], nsp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff6e3883d60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2021-07-14 20:49:16,927 WARNING | AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff6e3883d60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff6e3883d60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2021-07-14 20:49:16,214 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-14 20:49:17,258 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "2021-07-14 20:49:19,482 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-14 20:49:19,496 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Training loss (for one batch) at step 0: 8.5206\n",
      "Seen so far: 2 samples\n",
      "Training loss (for one batch) at step 200: 8.1316\n",
      "Seen so far: 402 samples\n",
      "Training loss (for one batch) at step 400: 8.1357\n",
      "Seen so far: 802 samples\n",
      "Training loss (for one batch) at step 600: 7.1829\n",
      "Seen so far: 1202 samples\n",
      "Training loss (for one batch) at step 800: 7.5305\n",
      "Seen so far: 1602 samples\n",
      "Training loss (for one batch) at step 1000: 7.5063\n",
      "Seen so far: 2002 samples\n",
      "Training loss (for one batch) at step 1200: 7.3599\n",
      "Seen so far: 2402 samples\n",
      "Training loss (for one batch) at step 1400: 7.2644\n",
      "Seen so far: 2802 samples\n",
      "Training loss (for one batch) at step 1600: 7.2841\n",
      "Seen so far: 3202 samples\n",
      "Training acc over epoch: 0.0528\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "2021-07-14 20:50:14,658 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-14 20:50:14,671 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Validation acc: 0.0652\n",
      "Time taken: 62.08s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 6.8220\n",
      "Seen so far: 2 samples\n",
      "Training loss (for one batch) at step 200: 7.1051\n",
      "Seen so far: 402 samples\n",
      "Training loss (for one batch) at step 400: 7.3221\n",
      "Seen so far: 802 samples\n",
      "Training loss (for one batch) at step 600: 7.4694\n",
      "Seen so far: 1202 samples\n",
      "Training loss (for one batch) at step 800: 7.2983\n",
      "Seen so far: 1602 samples\n",
      "Training loss (for one batch) at step 1000: 7.2708\n",
      "Seen so far: 2002 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tf_train_dataset = tf_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "tf_eval_dataset = tf_eval_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(tf_train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batchsize_for_testing))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in tf_eval_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def square_ragged_tensors(examples, key):\n",
    "    if isinstance(examples, dict):\n",
    "        return  examples[key].to_tensor(0)\n",
    "    else:\n",
    "        return examples.to_tensor(0)\n",
    "\n",
    "def some_func(examples: tf.data.Dataset, is_ragged=False) -> tf.data.Dataset:\n",
    "    if is_ragged:\n",
    "        if isinstance(list(examples)[0], dict):\n",
    "            for k_idx in range(len(list(examples)[0].keys())):\n",
    "                key = list(list(examples)[0].keys())[k_idx]\n",
    "                if isinstance(list(examples)[0][key], tf.RaggedTensor):\n",
    "                    list(examples)[0][key] = examples.batch(2).map(lambda x: square_ragged_tensors(x, key)).unbatch()\n",
    "        else:\n",
    "            examples = examples.batch(2).map(square_ragged_tensors).unbatch()\n",
    "        \n",
    "    return examples\n",
    "        \n",
    "a = tf.constant([0, 1, 2, 3])\n",
    "b = tf.constant([0, 1, 2, 3, 5, 6])\n",
    "rg = tf.ragged.stack([a, b])\n",
    "ex = {\"input_ids\": rg, \"hello_world\": [\"goodbye\", \"world\"]}\n",
    "\n",
    "batch_encodings = tf.data.Dataset.from_tensor_slices(ex)\n",
    "\n",
    "list(some_func(batch_encodings, True))\n",
    "# ex[\"input_ids\"].to_tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data['log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = test_data.iloc[0]['log']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = w2vp.bert_tokenizer.text_to_sequence(sentence).tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert = deepcopy(w2vp.BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert.get_output_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.save(SOURCE + '/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITSJA5hWASDn",
    "0JtgL-iQkqj2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "LongRunTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
