{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/assets\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-16 12:11:35.341827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Base -- #\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import yaml\n",
    "import csv\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "from yaspin import yaspin\n",
    "from functools import partial\n",
    "\n",
    "# -- Tokenizer -- #\n",
    "import tokenizers\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# -- PreTrained BERT -- #\n",
    "from transformers import create_optimizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import Dataset\n",
    "\n",
    "# -- Metrics -- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- Misc Models -- #\n",
    "import drain3\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- Custom -- #\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertForPreTraining\n",
    "from libs.transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from libs.transformers.src.transformers.modeling_tf_utils import shape_list\n",
    "from libs.transformers.src.transformers.data.data_tf_collator import TFDataCollatorForLanguageModeling\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sqlite_to_csv(inputFolder, ext, tableName):\n",
    "    \"\"\" inputFolder - Folder where sqlite files are located. \n",
    "        ext - Extension of your sqlite file (eg. db, sqlite, sqlite3 etc.)\n",
    "        tableName - table name from which you want to select the data.\n",
    "    \"\"\"\n",
    "    csvWriter = csv.writer(open(inputFolder+'/output.csv', 'w', newline=''))\n",
    "    for file1 in os.listdir(inputFolder):\n",
    "        if file1.endswith('.'+ext):\n",
    "            conn = sql.connect(inputFolder+'/'+file1)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM \"+tableName)\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                csvWriter.writerow(row)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-16 12:11:37.314334: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-16 12:11:37.315617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-16 12:11:37.366428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-16 12:11:37.366871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2021-07-16 12:11:37.366889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-16 12:11:37.378942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-16 12:11:37.378993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-16 12:11:37.386771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-16 12:11:37.390281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-16 12:11:37.402903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-16 12:11:37.406243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-16 12:11:37.407313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-16 12:11:37.407457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-16 12:11:37.408127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-16 12:11:37.408829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SOURCE = '/home/' + os.environ['USER']\n",
    "CONTAINER = 'core.soaesb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s | %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def database_builder(path: str) -> pd.DataFrame():\n",
    "    logger.info('Building DataFrame ...')\n",
    "    (_, _, files) = next(os.walk(path))\n",
    "    sql_query = 'SELECT * FROM logs'\n",
    "    data = []\n",
    "    for f in files:\n",
    "        if '.db' in f:\n",
    "            conn = create_connection(path + f)\n",
    "            d = pd.read_sql_query(sql_query, conn)\n",
    "            data.append(d)\n",
    "    logger.info('...complete!')\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def create_connection(path: str) -> sql.Connection:\n",
    "    \"\"\"\n",
    "    Creates a database connection\n",
    "    :param path: str\n",
    "        path to database object\n",
    "    :return sql.Connection\n",
    "        a connection to the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sql.connect(path)\n",
    "        logger.info('Connected to database ' + path)\n",
    "        return conn\n",
    "    except sql.Error as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 12:11:37,850 INFO | Building DataFrame ...\n",
      "2021-07-16 12:11:37,855 INFO | Connected to database /home/jovyan/data/elastic_logs.db\n",
      "2021-07-16 12:11:38,468 INFO | ...complete!\n"
     ]
    }
   ],
   "source": [
    "dataset = database_builder(SOURCE + '/data/')\n",
    "container_dataset = dataset[dataset['container_name'] == CONTAINER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_attributes(self, config: dict):\n",
    "    try:\n",
    "        config = config[self.__class__.__name__]\n",
    "    except Exception as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning('No configuration found for ' +\n",
    "                       self.__class__.__name__)\n",
    "\n",
    "    for attr in config.keys():\n",
    "        setattr(self, attr, config[attr])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingGlobalConfig:\n",
    "    embed_size: int = 512\n",
    "    max_vocab_size: int = 2000\n",
    "    buffer_size: int = 10000\n",
    "    global_training: bool = True\n",
    "    path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhraserModelConfig:\n",
    "    min_count: int = 5\n",
    "    threshold: float = 7\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'phrase_model.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClusteringConfig:\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'template_miner.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class PreprocessingPipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.PreprocessingGlobalConfig = PreprocessingGlobalConfig()\n",
    "        self.PhraserModelConfig = PhraserModelConfig()\n",
    "        self.TextClusteringConfig = TextClusteringConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                preprocessing_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self.PreprocessingGlobalConfig.load(preprocessing_config)\n",
    "        self.PhraserModelConfig.load(preprocessing_config)\n",
    "        self.TextClusteringConfig.load(preprocessing_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PrimeTokenizer:\n",
    "    def __init__(self, max_seq_length: int):\n",
    "        self.prime_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.prime_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "        self.prime_tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.prime_tokenizer.decoder = decoders.WordPiece()\n",
    "        self.prime_tokenizer.enable_padding(length=max_seq_length)\n",
    "        self.prime_tokenizer.enable_truncation(max_seq_length)\n",
    "\n",
    "        self.prime_tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.trainer = WordPieceTrainer(\n",
    "            vocab_size=153411,\n",
    "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "    def text_to_sequence(self, input_) -> List[tokenizers.Encoding]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.encode_batch(input_)\n",
    "        return self.prime_tokenizer.encode(input_)\n",
    "\n",
    "    def sequence_to_text(self, input_) -> List[str]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.decode_batch(batch)\n",
    "        return self.prime_tokenizer.decode(input_)\n",
    "\n",
    "    def train(self, data):\n",
    "        log_itr = iter(data)\n",
    "        self.prime_tokenizer.train_from_iterator(log_itr, self.trainer)\n",
    "        self.save()\n",
    "\n",
    "    def get_tokenizer(self) -> Tokenizer:\n",
    "        return self.prime_tokenizer\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.prime_tokenizer.get_vocab()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.prime_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def save(self):\n",
    "        self.prime_tokenizer.save(SOURCE + \"/results/prime_tokenizer.json\")\n",
    "        \n",
    "    def load(self):\n",
    "        self.prime_tokenizer = Tokenizer.from_file(SOURCE + \"/results/prime_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "#     if not os.path.exists(path):\n",
    "#         return\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "#     elif os.path.isdir(path):\n",
    "#         shutil.rmtree(path)\n",
    "#         return\n",
    "\n",
    "    joblib.dump(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhraseCaptureLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PhraserModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: PhraserModelConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(PhraserModel, self).__init__()\n",
    "        self.min_count = config.min_count\n",
    "        self.threshold = config.threshold\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model:\n",
    "            self.phrase_model = joblib.load(SOURCE +\n",
    "                                            self.path +\n",
    "                                            self.model_name)\n",
    "        else:\n",
    "            self.phrase_model = Phrases(min_count=self.min_count,\n",
    "                                        threshold=self.threshold)\n",
    "\n",
    "    def __call__(self, corpus: pd.DataFrame, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        def reorganize_return(corpus_with_phrases):\n",
    "            log_list = []\n",
    "            for tokenized_log in corpus_with_phrases:\n",
    "                log_list.append(' '.join(tokenized_log))\n",
    "            return log_list\n",
    "\n",
    "        split_corpus = [log.split(' ') for log in corpus['log']]\n",
    "\n",
    "        corpus_with_phrases = None\n",
    "        if not training:\n",
    "            frozen_model = self.phrase_model.freeze()\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "        else:\n",
    "            self.phrase_model.add_vocab(split_corpus)\n",
    "\n",
    "            if self.save_model:\n",
    "                save_model(self.phrase_model, SOURCE + self.path + self.model_name)\n",
    "\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "            \n",
    "        return reorganize_return(corpus_with_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextClusteringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextClustering:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: TextClusteringConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(TextClustering, self).__init__()\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model is True:\n",
    "            self.template_miner = joblib.load(SOURCE +\n",
    "                                              self.path +\n",
    "                                              self.model_name)\n",
    "        else:\n",
    "            self.template_miner = drain3.TemplateMiner()\n",
    "\n",
    "    def __call__(self, corpus: list, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        if training:\n",
    "            for log in corpus:\n",
    "                self.template_miner.add_log_message(log)\n",
    "            if self.save_model:\n",
    "                save_model(self.template_miner,\n",
    "                           SOURCE + self.path + self.model_name)\n",
    "\n",
    "            for idx, log in enumerate(corpus):\n",
    "                template = self.template_miner.match(log).get_template()\n",
    "                corpus[idx] = template\n",
    "\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster) for cluster in corpus]\n",
    "        else:\n",
    "#             log_list = self.get_unique_templates()\n",
    "#             print(f'Length of the log list: {len(log_list)}')\n",
    "#             return log_list\n",
    "            log_list = []\n",
    "            log_set = set()\n",
    "            for log in corpus:\n",
    "                match_cluster = self.template_miner.match(log)\n",
    "                if match_cluster is None:\n",
    "                    match_cluster = self.template_miner.add_log_message(log)['template_mined']\n",
    "                    log_set.add(match_cluster)\n",
    "                else:\n",
    "                    log_set.add(match_cluster.get_template())\n",
    "        \n",
    "#             l = [re.sub(pattern=r' +',\n",
    "#                            repl=' ',\n",
    "#                            string=cluster) for cluster in log_list]\n",
    "            return list(log_set)\n",
    "        \n",
    "    def get_unique_templates(self) -> list:\n",
    "        template_list = []\n",
    "        for cluster in self.template_miner.drain.clusters:\n",
    "            template_list.append(cluster.get_template())\n",
    "        return [re.sub(pattern=r' +',\n",
    "                       repl=' ',\n",
    "                       string=cluster) for cluster in template_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_all_batches(n_iter, log_labels, batch_size):\n",
    "    batches = []\n",
    "\n",
    "    for idx in range(n_iter + 1):\n",
    "        log_batch, labels = process_batch(dataset, idx, log_labels, batch_size)\n",
    "\n",
    "        batches.append((log_batch, labels))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def process_batch(dataset: pd.DataFrame,\n",
    "                  idx: int,\n",
    "                  labels: dict,\n",
    "                  batch_size: int) -> tuple:\n",
    "    start_window = idx * batch_size\n",
    "    end_window = (idx + 1) * batch_size\n",
    "    batched_data = dataset.iloc[start_window:end_window]\n",
    "    encoded_batch = prime_tokenizer.text_to_sequence(batched_data['log'].to_list())\n",
    "    id_batch = [log.ids for log in encoded_batch]\n",
    "#     y_batch = labels[batched_data['label']]\n",
    "    y_batch = [labels[idx] for idx in batched_data['label']]\n",
    "\n",
    "    tf_idf = tf.convert_to_tensor(id_batch, dtype=tf.float32)\n",
    "    y_idf  = tf.convert_to_tensor(y_batch, dtype=tf.float32)\n",
    "    \n",
    "    return tf_idf, y_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EncodedSeq = List[int]\n",
    "\n",
    "def normalize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    # remove timestamps and double spaces\n",
    "    regexp = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:               # Match all enclosed\n",
    "        \\d{4}-\\d{2}-\\d{2} # YYYY-MM-DD\n",
    "        [\\sT]             # Accept either a space or T\n",
    "        \\d{2}:\\d{2}:\\d{2} # HH:MM:SS\n",
    "        ([.,]\\d{3}|\\s)    # Accept either a space or milliseconds\n",
    "        )                 # End timestamp match\n",
    "        | (?:\\s{2,})      # Remove double spaces   \n",
    "        | [^a-zA-Z\\d:]    # Clean non-alphanumeric characters\n",
    "        \"\"\", re.X)        # re.X enables comments and whitespace\n",
    "\n",
    "    c_logs = deepcopy(logs)\n",
    "    c_logs.loc[:, 'log'].replace(\n",
    "        to_replace=regexp, \n",
    "        value=' ', \n",
    "        regex=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return c_logs\n",
    "\n",
    "\n",
    "def extract_unique_labels(logs: pd.DataFrame) -> dict:\n",
    "    # -- Labels -- #\n",
    "    label_unique = logs['label'].unique()\n",
    "    binary_labels = LabelEncoder().fit_transform(label_unique)\n",
    "\n",
    "    log_labels = {}\n",
    "    # TODO: This seems a bit messy, could it be cleaned up? \n",
    "    for idx, label in enumerate(label_unique):\n",
    "        log_labels.update({\n",
    "            label: binary_labels[idx]\n",
    "        })\n",
    "    return log_labels\n",
    "\n",
    "def create_sentence_pairing(examples):\n",
    "    first_seqs = []\n",
    "    nsp_labels = []\n",
    "\n",
    "    examples[\"log\"] = [\n",
    "        line for line in examples[\"log\"] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    \n",
    "    log_list = list(examples['log'])\n",
    "    for idx in range(len(log_list)):\n",
    "        first_value = log_list[idx]\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "\n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            second_value = log_list[rand_idx]\n",
    "\n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "\n",
    "        first_seqs.append((first_value, second_value))\n",
    "\n",
    "    return {\"log\": first_seqs, \"next_sentence_label\": nsp_labels}\n",
    "\n",
    "\n",
    "def generate_test_train_split(tokenized_datasets, test_size=.30):\n",
    "    # Train - Test Split\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        list(range(len(tokenized_datasets))), test_size=test_size\n",
    "    )\n",
    "\n",
    "    test_dataset = tokenized_datasets.select(test_indices)\n",
    "    train_dataset = tokenized_datasets.select(train_indices)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "class UnsupervisedLearningPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PreprocessingPipelineConfig, \n",
    "        epochs: int = 3, \n",
    "        batch_size: int = 50, \n",
    "        seq_length: int = 200\n",
    "    ) -> None:\n",
    "\n",
    "        # Logs\n",
    "        self.normalized_logs = None\n",
    "        self.log_labels = None\n",
    "        self.logs_with_phrases = list()\n",
    "        \n",
    "        # Tokenizers\n",
    "        self.bert_tokenizer = PrimeTokenizer(seq_length)\n",
    "        self.fast_tokenizer = None\n",
    "        \n",
    "        # Dataset for Training/Evaluation\n",
    "        self.data_collator = None\n",
    "        self.tf_train_dataset = None\n",
    "        self.tf_test_dataset = None\n",
    "        self.token_logs = None\n",
    "        \n",
    "        # BERT Model\n",
    "        self.bert_config = None\n",
    "        self.BERT = None\n",
    "        self.optimizer = None\n",
    "        self.lr_schedule = None\n",
    "        self.loss_fn = None\n",
    "        self.train_acc_metric = None\n",
    "        self.val_acc_metric = None\n",
    "        \n",
    "        # Models\n",
    "        self.pm = PhraserModel(config.PhraserModelConfig, \n",
    "                               config.PreprocessingGlobalConfig)\n",
    "        self.tc = TextClustering(config.TextClusteringConfig,\n",
    "                                 config.PreprocessingGlobalConfig)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.max_seq_len = seq_length\n",
    "        self.n_logs = 0\n",
    "        self.n_iter = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_per_epoch = None\n",
    "        self.logs_as_templates = None\n",
    "        \n",
    "\n",
    "    def initialize_fast_tokenizer(self):\n",
    "        tokenizer_obj = self.bert_tokenizer.get_tokenizer()\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj)\n",
    "        fast_tokenizer.model_max_length = self.max_seq_len\n",
    "        fast_tokenizer.unk_token = \"[UNK]\"\n",
    "        fast_tokenizer.sep_token = \"[SEP]\"\n",
    "        fast_tokenizer.pad_token = \"[PAD]\"\n",
    "        fast_tokenizer.cls_token = \"[CLS]\"\n",
    "        fast_tokenizer.mask_token = \"[MASK]\"\n",
    "        self.fast_tokenizer = fast_tokenizer\n",
    "        return fast_tokenizer\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_data_signatures(train_dataset, test_dataset):\n",
    "        # Train Signatures\n",
    "        train_signature = {\n",
    "            feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "            for feature in train_dataset.features\n",
    "            if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "        }\n",
    "        train_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "        train_signature = (train_signature, {\"labels\": train_signature[\"labels\"], \"next_sentence_label\": train_signature[\"next_sentence_label\"]})\n",
    "        \n",
    "        # Test Signatures\n",
    "        test_signature = {\n",
    "            feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "            for feature in test_dataset.features\n",
    "            if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "        }\n",
    "        test_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        test_signature[\"labels\"] = test_signature[\"input_ids\"]\n",
    "        test_signature = (test_signature, {\"labels\": test_signature[\"labels\"], \"next_sentence_label\": test_signature[\"next_sentence_label\"]})\n",
    "        \n",
    "        return train_signature, test_signature\n",
    "    \n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        # Remove empty lines\n",
    "        return self.fast_tokenizer(\n",
    "            examples[\"log\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_pre_training_data(self):\n",
    "        fast_tokenizer = self.initialize_fast_tokenizer()\n",
    "        \n",
    "        dt = self.normalized_logs.drop([\"label\", \"container_name\", \"timestamp\"], axis=1)\n",
    "        data = Dataset.from_pandas(dt)\n",
    "        \n",
    "        tokenized_datasets = data.map(\n",
    "            create_sentence_pairing,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"log\"],\n",
    "            desc=\"Creating sentence pairings for NSP Head\"\n",
    "        )\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"log\"],\n",
    "            desc=\"Running tokenizer on dataset line_by_line\"\n",
    "        )\n",
    "        \n",
    "        train_dataset, test_dataset = generate_test_train_split(tokenized_datasets)\n",
    "        train_signature, test_signature = self.generate_data_signatures(train_dataset, test_dataset)\n",
    "        \n",
    "        self.batches_per_epoch = len(train_dataset) // self.batch_size\n",
    "        \n",
    "        data_collator = TFDataCollatorForLanguageModeling(\n",
    "            tokenizer=self.fast_tokenizer,\n",
    "            padding_length=self.max_seq_len,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        \n",
    "        tokenized_generator = partial(data_collator, train_dataset, fast_tokenizer)\n",
    "        test_generator = partial(data_collator, test_dataset, fast_tokenizer)\n",
    "        \n",
    "        self.tf_train_dataset = (\n",
    "            tf.data.Dataset.from_generator(tokenized_generator, output_signature=train_signature)\n",
    "            .batch(batch_size=self.batch_size, drop_remainder=True)\n",
    "            .repeat(int(5))\n",
    "        )\n",
    "        \n",
    "        self.tf_test_dataset = (\n",
    "            tf.data.Dataset.from_generator(test_generator, output_signature=test_signature)\n",
    "            .batch(batch_size=self.batch_size, drop_remainder=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_bert_tokenizer(self, load_model=False):\n",
    "        if load_model:\n",
    "            self.bert_tokenizer.load()\n",
    "        else:\n",
    "            self.bert_tokenizer.train(self.logs_as_templates)\n",
    "            \n",
    "            \n",
    "    def compute_loss(self, labels, logits):\n",
    "        # make sure only labels that are not equal to -100\n",
    "        # are taken into account as loss\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n",
    "            mask=masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n",
    "        )\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "\n",
    "        return (masked_lm_labels, masked_lm_reduced_logits), (next_sentence_label, next_sentence_reduced_logits)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.BERT(x, training=True)\n",
    "            mlm, nsp = self.compute_loss(y, (logits[\"prediction_logits\"], logits[\"seq_relationship_logits\"]))\n",
    "            masked_lm_loss = self.loss_fn(mlm[0], mlm[1])\n",
    "            next_sentence_loss = self.loss_fn(nsp[0], nsp[1])\n",
    "            loss_value = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.BERT.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.BERT.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        self.train_acc_metric.update_state(mlm[0], mlm[1])\n",
    "        self.train_acc_metric.update_state(nsp[0], nsp[1])\n",
    "\n",
    "        return loss_value\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x, y):\n",
    "        val_logits = self.BERT(x, training=False)\n",
    "        mlm, nsp = self.compute_loss(y, (val_logits[\"prediction_logits\"], val_logits[\"seq_relationship_logits\"]))\n",
    "        # Update val metrics\n",
    "        self.val_acc_metric.update_state(mlm[0], mlm[1])\n",
    "        self.val_acc_metric.update_state(nsp[0], nsp[1])\n",
    "        \n",
    "        \n",
    "    def prepare_bert_model(self):\n",
    "        self.bert_config = BertConfig(\n",
    "            vocab_size=self.bert_tokenizer.get_vocab_size(),\n",
    "            hidden_size=512,\n",
    "            num_hidden_layers=8,\n",
    "            num_attention_heads=8\n",
    "        )\n",
    "\n",
    "        self.optimizer, self.lr_schedule = create_optimizer(\n",
    "            init_lr=1e-4,\n",
    "            num_train_steps=int(5 * self.batches_per_epoch),\n",
    "            num_warmup_steps=2,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=0.1,\n",
    "            weight_decay_rate=0.01,\n",
    "        )\n",
    "        \n",
    "        self.BERT = TFBertForPreTraining(self.bert_config)\n",
    "        self.BERT.resize_token_embeddings(len(self.fast_tokenizer))\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Prepare the metrics.\n",
    "        self.train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "\n",
    "    def pre_train_bert(self):\n",
    "        # Prepare BERT Model for Pre-Training\n",
    "        self.prepare_bert_model()\n",
    "        \n",
    "        # Prefetch Data\n",
    "        self.tf_train_dataset = self.tf_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        self.tf_test_dataset = self.tf_test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        with yaspin(text=\"\", color='blue') as sp:\n",
    "            # Training BERT\n",
    "            for epoch in range(self.epochs):\n",
    "                print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Iterate over the batches of the dataset.\n",
    "                for step, (x_batch_train, y_batch_train) in enumerate(self.tf_train_dataset):\n",
    "                    loss_value = self.train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "                    # Log every 200 batches.\n",
    "                    if step % 200 == 0:\n",
    "                        print(\n",
    "                            \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                            % (step, float(loss_value))\n",
    "                        )\n",
    "                        print(\"Seen so far: %d samples\" % ((step + 1) * self.batch_size))\n",
    "\n",
    "                # Display metrics at the end of each epoch.\n",
    "                train_acc = self.train_acc_metric.result()\n",
    "                print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "                # Reset training metrics at the end of each epoch\n",
    "                self.train_acc_metric.reset_states()\n",
    "\n",
    "                # Run an evaluation loop at the end of each epoch.\n",
    "                for x_batch_test, y_batch_test in self.tf_test_dataset:\n",
    "                    self.test_step(x_batch_test, y_batch_test)\n",
    "                eval_acc = self.val_acc_metric.result()\n",
    "                self.val_acc_metric.reset_states()\n",
    "                print(\"Testing acc: %.4f\" % (float(eval_acc),))\n",
    "                print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "            \n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Finished Training Epochs')\n",
    "            \n",
    "    \n",
    "    def run_bert(self, batch):\n",
    "        return self.BERT(batch,\n",
    "                         output_attentions=True)\n",
    "\n",
    "    def fit(self, logs: pd.DataFrame):\n",
    "        assert len(logs.index) > 0, 'process received an empty dataframe!'\n",
    "        \n",
    "        with yaspin(text=\"Normalizing Logs\", color='green') as sp:\n",
    "            self.normalized_logs = normalize_logs(logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log normalization')\n",
    "            \n",
    "            sp.text = \"Extracting phrases\"\n",
    "            self.logs_with_phrases = self.pm(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed phrase extraction')\n",
    "            \n",
    "            sp.text = \"Converting to log templates\"\n",
    "            self.logs_as_templates = np.array(self.tc(self.logs_with_phrases))\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log template conversion')\n",
    "        \n",
    "            sp.text = \"Extracting Unique Labels\"\n",
    "            self.log_labels = extract_unique_labels(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed extracting unique labels')\n",
    "\n",
    "            sp.text = \"Training Tokenizer\"\n",
    "            self.train_bert_tokenizer()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed training of custom tokenizer')\n",
    "                        \n",
    "            sp.text = \"Processing training dataset\"\n",
    "            self.get_pre_training_data()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed processing training dataset')\n",
    "            \n",
    "            sp.text = \"Pretraining BERT\"\n",
    "            self.pre_train_bert()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed BERT pretraining')\n",
    "            \n",
    "        \n",
    "    def transform(self, batch: pd.DataFrame):\n",
    "        x = self.normalize_logs(batch)\n",
    "        x = self.pm(x, False)\n",
    "        x = list(self.tc(x, False))\n",
    "        x = self.serve_batch(x)\n",
    "        return self.run_bert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V Pipeline Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_path = SOURCE + '/assets/notebooks/PreprocessingConfig.yaml'\n",
    "preprocessing_config = PreprocessingPipelineConfig()\n",
    "preprocessing_config.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 12:35:07,306 INFO | Starting Drain3 template miner\n",
      "2021-07-16 12:35:07,307 INFO | Loading configuration from drain3.ini\n",
      "\u001b[K\u001b[32m✔ Completed log normalization\u001b[0m \n",
      "2021-07-16 12:35:07,343 INFO | exporting phrases from Phrases<0 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "2021-07-16 12:35:07,343 INFO | FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<0 phrases, min_count=5, threshold=7> from Phrases<0 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.00s', 'datetime': '2021-07-16T12:35:07.343740', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \\n[GCC 9.3.0]', 'platform': 'Linux-5.8.0-55-generic-x86_64-with-glibc2.10', 'event': 'created'}\n",
      "\u001b[K\u001b[32m✔ Completed phrase extraction\u001b[0m \n",
      "\u001b[K\u001b[32m✔ Completed log template conversion\u001b[0m \n",
      "\u001b[K\u001b[32m✔ Completed extracting unique labels\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\u001b[K\u001b[32m✔ Completed training of custom tokenizer\u001b[0m \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa060d21e1d457ca8800c58ac2c956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating sentence pairings for NSP Head:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4ea90a22ac4794937a7b29b31799b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset line_by_line:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[32m✔ Completed processing training dataset\u001b[0m \n",
      "\u001b[34m⠋\u001b[0m \u001b[K\n",
      "Start of epoch 0\n",
      "\u001b[34m⠙\u001b[0m \u001b[KWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "2021-07-16 12:35:12,832 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-16 12:35:12,847 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\u001b[34m⠇\u001b[0m \u001b[KWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "2021-07-16 12:35:14,236 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-16 12:35:14,251 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining loss (for one batch) at step 0: 8.6040\n",
      "Seen so far: 2 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining loss (for one batch) at step 200: 8.5414\n",
      "Seen so far: 402 samples\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining loss (for one batch) at step 400: 8.0799\n",
      "Seen so far: 802 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining loss (for one batch) at step 600: 7.7189\n",
      "Seen so far: 1202 samples\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining loss (for one batch) at step 800: 7.2637\n",
      "Seen so far: 1602 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining loss (for one batch) at step 1000: 8.0167\n",
      "Seen so far: 2002 samples\n",
      "\u001b[34m⠧\u001b[0m \u001b[KTraining loss (for one batch) at step 1200: 7.4470\n",
      "Seen so far: 2402 samples\n",
      "\u001b[34m⠹\u001b[0m \u001b[KTraining loss (for one batch) at step 1400: 7.0914\n",
      "Seen so far: 2802 samples\n",
      "\u001b[34m⠧\u001b[0m \u001b[KTraining loss (for one batch) at step 1600: 7.5767\n",
      "Seen so far: 3202 samples\n",
      "\u001b[34m⠹\u001b[0m \u001b[KTraining acc over epoch: 0.0574\n",
      "\u001b[34m⠸\u001b[0m \u001b[KWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "2021-07-16 12:36:09,645 WARNING | The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2021-07-16 12:36:09,658 WARNING | The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\u001b[34m⠋\u001b[0m \u001b[KValidation acc: 0.0624\n",
      "Time taken: 59.09s\n",
      "\n",
      "Start of epoch 1\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 0: 7.5593\n",
      "Seen so far: 2 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 200: 7.3578\n",
      "Seen so far: 402 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 400: 7.3711\n",
      "Seen so far: 802 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 600: 7.1798\n",
      "Seen so far: 1202 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 800: 7.0991\n",
      "Seen so far: 1602 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 1000: 7.2705\n",
      "Seen so far: 2002 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 1200: 7.4742\n",
      "Seen so far: 2402 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 1400: 7.3942\n",
      "Seen so far: 2802 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 1600: 7.2994\n",
      "Seen so far: 3202 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining acc over epoch: 0.0613\n",
      "\u001b[34m⠴\u001b[0m \u001b[KValidation acc: 0.0593\n",
      "Time taken: 54.39s\n",
      "\n",
      "Start of epoch 2\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 0: 7.5140\n",
      "Seen so far: 2 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 200: 7.2062\n",
      "Seen so far: 402 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 400: 7.7677\n",
      "Seen so far: 802 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 600: 7.8756\n",
      "Seen so far: 1202 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 800: 7.7088\n",
      "Seen so far: 1602 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 1000: 7.5388\n",
      "Seen so far: 2002 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 1200: 7.2800\n",
      "Seen so far: 2402 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining loss (for one batch) at step 1400: 8.2184\n",
      "Seen so far: 2802 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 1600: 6.6885\n",
      "Seen so far: 3202 samples\n",
      "\u001b[34m⠏\u001b[0m \u001b[KTraining acc over epoch: 0.0617\n",
      "\u001b[34m⠋\u001b[0m \u001b[KValidation acc: 0.0608\n",
      "Time taken: 55.18s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 7.4354\n",
      "Seen so far: 2 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 200: 7.4804\n",
      "Seen so far: 402 samples\n",
      "\u001b[34m⠹\u001b[0m \u001b[KTraining loss (for one batch) at step 400: 7.5570\n",
      "Seen so far: 802 samples\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining loss (for one batch) at step 600: 7.4448\n",
      "Seen so far: 1202 samples\n",
      "\u001b[34m⠼\u001b[0m \u001b[KTraining loss (for one batch) at step 800: 7.2061\n",
      "Seen so far: 1602 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 1000: 7.2918\n",
      "Seen so far: 2002 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 1200: 7.3297\n",
      "Seen so far: 2402 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 1400: 7.1963\n",
      "Seen so far: 2802 samples\n",
      "\u001b[34m⠧\u001b[0m \u001b[KTraining loss (for one batch) at step 1600: 7.4216\n",
      "Seen so far: 3202 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining acc over epoch: 0.0617\n",
      "\u001b[34m⠸\u001b[0m \u001b[KValidation acc: 0.0619\n",
      "Time taken: 55.11s\n",
      "\n",
      "Start of epoch 4\n",
      "\u001b[34m⠼\u001b[0m \u001b[KTraining loss (for one batch) at step 0: 7.4629\n",
      "Seen so far: 2 samples\n",
      "\u001b[34m⠋\u001b[0m \u001b[KTraining loss (for one batch) at step 200: 7.3955\n",
      "Seen so far: 402 samples\n",
      "\u001b[34m⠦\u001b[0m \u001b[KTraining loss (for one batch) at step 400: 7.1919\n",
      "Seen so far: 802 samples\n",
      "\u001b[34m⠹\u001b[0m \u001b[KTraining loss (for one batch) at step 600: 7.5124\n",
      "Seen so far: 1202 samples\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining loss (for one batch) at step 800: 7.2475\n",
      "Seen so far: 1602 samples\n",
      "\u001b[34m⠸\u001b[0m \u001b[KTraining loss (for one batch) at step 1000: 7.1568\n",
      "Seen so far: 2002 samples\n",
      "\u001b[34m⠏\u001b[0m \u001b[KTraining loss (for one batch) at step 1200: 7.2996\n",
      "Seen so far: 2402 samples\n",
      "\u001b[34m⠴\u001b[0m \u001b[KTraining loss (for one batch) at step 1400: 7.3207\n",
      "Seen so far: 2802 samples\n",
      "\u001b[34m⠙\u001b[0m \u001b[KTraining loss (for one batch) at step 1600: 7.2698\n",
      "Seen so far: 3202 samples\n",
      "\u001b[34m⠇\u001b[0m \u001b[KTraining acc over epoch: 0.0619\n",
      "\u001b[34m⠧\u001b[0m \u001b[KValidation acc: 0.0603\n",
      "Time taken: 55.14s\n",
      "\u001b[K\u001b[34m✔ Finished Training Epochs\u001b[0m \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_101/586582923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mw2vp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnsupervisedLearningPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtraining_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2vp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_101/1136450512.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Pretraining BERT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_train_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'✔ Completed BERT pretraining'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_101/1136450512.py\u001b[0m in \u001b[0;36mpre_train_bert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'✔ Finished Training Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# -- Unsupervised Learning Pipeline -- #\n",
    "\n",
    "'''\n",
    "Input: pd.DataFrame with batch_size number of rows \n",
    "Seq: \n",
    "    Normalize \n",
    "    Phraser\n",
    "    Clustering\n",
    "    Extract Unique Layers\n",
    "    BERT\n",
    "Returns: transformers.TFBertForPreTrainingOutput\n",
    "'''\n",
    "\n",
    "# --- SUBWORD TOKENIZER --\n",
    "w2vp = UnsupervisedLearningPipeline(preprocessing_config, epochs=5, batch_size=2, seq_length=300)\n",
    "training_outputs = w2vp.fit(dataset[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in w2vp.tf_train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data['log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = test_data.iloc[0]['log']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = w2vp.bert_tokenizer.text_to_sequence(sentence).tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert = deepcopy(w2vp.BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert.get_output_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.save(SOURCE + '/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITSJA5hWASDn",
    "0JtgL-iQkqj2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "LongRunTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
