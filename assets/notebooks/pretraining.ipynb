{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/assets\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 14:03:03.962459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Base -- #\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import yaml\n",
    "import csv\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "from yaspin import yaspin\n",
    "from functools import partial\n",
    "\n",
    "# -- Tokenizer -- #\n",
    "import tokenizers\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# -- PreTrained BERT -- #\n",
    "from transformers import create_optimizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import Dataset\n",
    "\n",
    "# -- Metrics -- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- Misc Models -- #\n",
    "import drain3\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- Custom -- #\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertForPreTraining\n",
    "from libs.transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from libs.transformers.src.transformers.modeling_tf_utils import shape_list\n",
    "from libs.transformers.src.transformers.data.data_tf_collator import TFDataCollatorForLanguageModeling\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_sqlite_to_csv(inputFolder, ext, tableName):\n",
    "    \"\"\" inputFolder - Folder where sqlite files are located. \n",
    "        ext - Extension of your sqlite file (eg. db, sqlite, sqlite3 etc.)\n",
    "        tableName - table name from which you want to select the data.\n",
    "    \"\"\"\n",
    "    csvWriter = csv.writer(open(inputFolder+'/output.csv', 'w', newline=''))\n",
    "    for file1 in os.listdir(inputFolder):\n",
    "        if file1.endswith('.'+ext):\n",
    "            conn = sql.connect(inputFolder+'/'+file1)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM \"+tableName)\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                csvWriter.writerow(row)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 14:03:05.308641: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-19 14:03:05.309188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-19 14:03:05.356495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-19 14:03:05.357129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2021-07-19 14:03:05.357145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-19 14:03:05.358685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-19 14:03:05.358716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-19 14:03:05.359336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-19 14:03:05.359491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-19 14:03:05.361007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-19 14:03:05.361360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-19 14:03:05.361445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-19 14:03:05.361556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-19 14:03:05.362169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-19 14:03:05.362684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SOURCE = '/home/' + os.environ['USER']\n",
    "CONTAINER = 'core.soaesb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s | %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def database_builder(path: str) -> pd.DataFrame():\n",
    "    logger.info('Building DataFrame ...')\n",
    "    (_, _, files) = next(os.walk(path))\n",
    "    sql_query = 'SELECT * FROM logs'\n",
    "    data = []\n",
    "    for f in files:\n",
    "        if '.db' in f:\n",
    "            conn = create_connection(path + f)\n",
    "            d = pd.read_sql_query(sql_query, conn)\n",
    "            data.append(d)\n",
    "    logger.info('...complete!')\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def create_connection(path: str) -> sql.Connection:\n",
    "    \"\"\"\n",
    "    Creates a database connection\n",
    "    :param path: str\n",
    "        path to database object\n",
    "    :return sql.Connection\n",
    "        a connection to the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sql.connect(path)\n",
    "        logger.info('Connected to database ' + path)\n",
    "        return conn\n",
    "    except sql.Error as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19 14:03:05,381 INFO | Building DataFrame ...\n",
      "2021-07-19 14:03:05,382 INFO | Connected to database /home/jovyan/data/elastic_logs.db\n",
      "2021-07-19 14:03:05,940 INFO | ...complete!\n"
     ]
    }
   ],
   "source": [
    "dataset = database_builder(SOURCE + '/data/')\n",
    "container_dataset = dataset[dataset['container_name'] == CONTAINER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_attributes(self, config: dict):\n",
    "    try:\n",
    "        config = config[self.__class__.__name__]\n",
    "    except Exception as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning('No configuration found for ' +\n",
    "                       self.__class__.__name__)\n",
    "\n",
    "    for attr in config.keys():\n",
    "        setattr(self, attr, config[attr])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingGlobalConfig:\n",
    "    embed_size: int = 512\n",
    "    max_vocab_size: int = 2000\n",
    "    buffer_size: int = 10000\n",
    "    global_training: bool = True\n",
    "    path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhraserModelConfig:\n",
    "    min_count: int = 5\n",
    "    threshold: float = 7\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'phrase_model.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClusteringConfig:\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'template_miner.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class PreprocessingPipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.PreprocessingGlobalConfig = PreprocessingGlobalConfig()\n",
    "        self.PhraserModelConfig = PhraserModelConfig()\n",
    "        self.TextClusteringConfig = TextClusteringConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                preprocessing_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self.PreprocessingGlobalConfig.load(preprocessing_config)\n",
    "        self.PhraserModelConfig.load(preprocessing_config)\n",
    "        self.TextClusteringConfig.load(preprocessing_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PrimeTokenizer:\n",
    "    def __init__(self, max_seq_length: int):\n",
    "        self.prime_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.prime_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "        self.prime_tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.prime_tokenizer.decoder = decoders.WordPiece()\n",
    "        self.prime_tokenizer.enable_padding(length=max_seq_length)\n",
    "        self.prime_tokenizer.enable_truncation(max_seq_length)\n",
    "\n",
    "        self.prime_tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.trainer = WordPieceTrainer(\n",
    "            vocab_size=153411,\n",
    "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "    def text_to_sequence(self, input_) -> List[tokenizers.Encoding]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.encode_batch(input_)\n",
    "        return self.prime_tokenizer.encode(input_)\n",
    "\n",
    "    def sequence_to_text(self, input_) -> List[str]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.decode_batch(batch)\n",
    "        return self.prime_tokenizer.decode(input_)\n",
    "\n",
    "    def train(self, data):\n",
    "        log_itr = iter(data)\n",
    "        self.prime_tokenizer.train_from_iterator(log_itr, self.trainer)\n",
    "        self.save()\n",
    "\n",
    "    def get_tokenizer(self) -> Tokenizer:\n",
    "        return self.prime_tokenizer\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.prime_tokenizer.get_vocab()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.prime_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def save(self):\n",
    "        self.prime_tokenizer.save(SOURCE + \"/results/prime_tokenizer.json\")\n",
    "        \n",
    "    def load(self):\n",
    "        self.prime_tokenizer = Tokenizer.from_file(SOURCE + \"/results/prime_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "#     if not os.path.exists(path):\n",
    "#         return\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "#     elif os.path.isdir(path):\n",
    "#         shutil.rmtree(path)\n",
    "#         return\n",
    "\n",
    "    joblib.dump(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhraseCaptureLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PhraserModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: PhraserModelConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(PhraserModel, self).__init__()\n",
    "        self.min_count = config.min_count\n",
    "        self.threshold = config.threshold\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model:\n",
    "            self.phrase_model = joblib.load(SOURCE +\n",
    "                                            self.path +\n",
    "                                            self.model_name)\n",
    "        else:\n",
    "            self.phrase_model = Phrases(min_count=self.min_count,\n",
    "                                        threshold=self.threshold)\n",
    "\n",
    "    def __call__(self, corpus: pd.DataFrame, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        def reorganize_return(corpus_with_phrases):\n",
    "            log_list = []\n",
    "            for tokenized_log in corpus_with_phrases:\n",
    "                log_list.append(' '.join(tokenized_log))\n",
    "            return log_list\n",
    "\n",
    "        split_corpus = [log.split(' ') for log in corpus['log']]\n",
    "\n",
    "        corpus_with_phrases = None\n",
    "        if not training:\n",
    "            frozen_model = self.phrase_model.freeze()\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "        else:\n",
    "            self.phrase_model.add_vocab(split_corpus)\n",
    "\n",
    "            if self.save_model:\n",
    "                save_model(self.phrase_model, SOURCE + self.path + self.model_name)\n",
    "\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "            \n",
    "        return reorganize_return(corpus_with_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextClusteringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextClustering:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: TextClusteringConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(TextClustering, self).__init__()\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model is True:\n",
    "            self.template_miner = joblib.load(SOURCE +\n",
    "                                              self.path +\n",
    "                                              self.model_name)\n",
    "        else:\n",
    "            self.template_miner = drain3.TemplateMiner()\n",
    "\n",
    "    def __call__(self, corpus: list, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        if training:\n",
    "            for log in corpus:\n",
    "                self.template_miner.add_log_message(log)\n",
    "            if self.save_model:\n",
    "                save_model(self.template_miner,\n",
    "                           SOURCE + self.path + self.model_name)\n",
    "\n",
    "            for idx, log in enumerate(corpus):\n",
    "                template = self.template_miner.match(log).get_template()\n",
    "                corpus[idx] = template\n",
    "\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster) for cluster in corpus]\n",
    "        else:\n",
    "#             log_list = self.get_unique_templates()\n",
    "#             print(f'Length of the log list: {len(log_list)}')\n",
    "#             return log_list\n",
    "            log_list = []\n",
    "            log_set = set()\n",
    "            for log in corpus:\n",
    "                match_cluster = self.template_miner.match(log)\n",
    "                if match_cluster is None:\n",
    "                    match_cluster = self.template_miner.add_log_message(log)['template_mined']\n",
    "                    log_set.add(match_cluster)\n",
    "                else:\n",
    "                    log_set.add(match_cluster.get_template())\n",
    "        \n",
    "#             l = [re.sub(pattern=r' +',\n",
    "#                            repl=' ',\n",
    "#                            string=cluster) for cluster in log_list]\n",
    "            return list(log_set)\n",
    "        \n",
    "    def get_unique_templates(self) -> list:\n",
    "        template_list = []\n",
    "        for cluster in self.template_miner.drain.clusters:\n",
    "            template_list.append(cluster.get_template())\n",
    "        return [re.sub(pattern=r' +',\n",
    "                       repl=' ',\n",
    "                       string=cluster) for cluster in template_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_all_batches(n_iter, log_labels, batch_size):\n",
    "    batches = []\n",
    "\n",
    "    for idx in range(n_iter + 1):\n",
    "        log_batch, labels = process_batch(dataset, idx, log_labels, batch_size)\n",
    "\n",
    "        batches.append((log_batch, labels))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def process_batch(dataset: pd.DataFrame,\n",
    "                  idx: int,\n",
    "                  labels: dict,\n",
    "                  batch_size: int) -> tuple:\n",
    "    start_window = idx * batch_size\n",
    "    end_window = (idx + 1) * batch_size\n",
    "    batched_data = dataset.iloc[start_window:end_window]\n",
    "    encoded_batch = prime_tokenizer.text_to_sequence(batched_data['log'].to_list())\n",
    "    id_batch = [log.ids for log in encoded_batch]\n",
    "    y_batch = [labels[idx] for idx in batched_data['label']]\n",
    "\n",
    "    tf_idf = tf.convert_to_tensor(id_batch, dtype=tf.float32)\n",
    "    y_idf  = tf.convert_to_tensor(y_batch, dtype=tf.float32)\n",
    "    \n",
    "    return tf_idf, y_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Helper classes\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
    "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
    "    # that saves the model with this method after each epoch.\n",
    "    def __init__(self, output_dir, model, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save_pretrained(self.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBert:\n",
    "    def __init__(self, vocab_size, batch_size, epochs, batches_per_epoch, token_len):\n",
    "        self.bert_config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=512,\n",
    "            num_hidden_layers=8,\n",
    "            num_attention_heads=8\n",
    "        )\n",
    "\n",
    "        self.optimizer, self.lr_schedule = create_optimizer(\n",
    "            init_lr=1e-4,\n",
    "            num_train_steps=int(5 * batches_per_epoch),\n",
    "            num_warmup_steps=2,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=0.1,\n",
    "            weight_decay_rate=0.01,\n",
    "        )\n",
    "        \n",
    "        self.BERT = TFBertForPreTraining(self.bert_config)\n",
    "        self.BERT.resize_token_embeddings(token_len)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Prepare the metrics.\n",
    "        self.train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.outputs = None\n",
    "        \n",
    "        # Save Model Callback\n",
    "        self.SavePretrainedModel = SavePretrainedCallback(f\"{SOURCE}/results/PreTrainedModel/\", self.BERT)\n",
    "        \n",
    "        # Checkpoint System\n",
    "        self.chkpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=self.optimizer, model=self.BERT)\n",
    "        self.chkpt_manager = tf.train.CheckpointManager(self.chkpt, f\"{SOURCE}/results/PreTrainedChkpts/tf_ckpts\", max_to_keep=3)\n",
    "        \n",
    "        \n",
    "        self.chkpt.restore(self.chkpt_manager.latest_checkpoint)\n",
    "        if self.chkpt_manager.latest_checkpoint:\n",
    "            print(\"\\nRestored from {}\".format(self.chkpt_manager.latest_checkpoint))\n",
    "        else:\n",
    "            print(\"\\nInitializing from scratch.\")\n",
    "\n",
    "        \n",
    "    def compute_loss(self, labels, logits):\n",
    "        # make sure only labels that are not equal to -100\n",
    "        # are taken into account as loss\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n",
    "            mask=masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n",
    "        )\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "\n",
    "        return (masked_lm_labels, masked_lm_reduced_logits), (next_sentence_label, next_sentence_reduced_logits)\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.BERT(x, training=True)\n",
    "            mlm, nsp = self.compute_loss(y, (logits[\"prediction_logits\"], logits[\"seq_relationship_logits\"]))\n",
    "            masked_lm_loss = self.loss_fn(mlm[0], mlm[1])\n",
    "            next_sentence_loss = self.loss_fn(nsp[0], nsp[1])\n",
    "            loss_value = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.BERT.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.BERT.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        self.train_acc_metric.update_state(mlm[0], mlm[1])\n",
    "        self.train_acc_metric.update_state(nsp[0], nsp[1])\n",
    "        \n",
    "        self.outputs = logits\n",
    "\n",
    "        return loss_value\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x, y):\n",
    "        val_logits = self.BERT(x, training=False)\n",
    "        mlm, nsp = self.compute_loss(y, (val_logits[\"prediction_logits\"], val_logits[\"seq_relationship_logits\"]))\n",
    "        # Update val metrics\n",
    "        self.val_acc_metric.update_state(mlm[0], mlm[1])\n",
    "        self.val_acc_metric.update_state(nsp[0], nsp[1])\n",
    "        \n",
    "        \n",
    "    def print(self, log, verbose):\n",
    "        if not verbose:\n",
    "            return\n",
    "        print(log)\n",
    "        \n",
    "        \n",
    "    def __call__(self, train_data, test_data, verbose=True):\n",
    "        # Training BERT\n",
    "        with yaspin(text=\"\", color='blue') as sp:\n",
    "            for epoch in range(self.epochs):\n",
    "                sp.text = f\"Epochs {epoch}/{self.epochs}\"\n",
    "                self.print(\"\\nStart of epoch %d\" % (epoch,), verbose)\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Iterate over the batches of the dataset.\n",
    "                for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
    "                    loss_value = self.train_step(x_batch_train, y_batch_train)\n",
    "                    \n",
    "                    # Increment and Save checkpoint for every 10 steps\n",
    "                    self.chkpt.step.assign_add(1)\n",
    "                    if int(self.chkpt.step) % 200 == 0:\n",
    "                        save_path = self.chkpt_manager.save()\n",
    "                        print(\"\\nSaved checkpoint for step {}: {}\".format(int(self.chkpt.step), save_path))\n",
    "                        print(\"\\nloss {:1.2f}\".format(loss_value))\n",
    "\n",
    "                    # Log every 200 batches.\n",
    "                    if step % 200 == 0:\n",
    "                        self.print(\n",
    "                            \"\\nTraining loss (for one batch) at step %d: %.4f\"\n",
    "                            % (step, float(loss_value)),\n",
    "                            verbose\n",
    "                        )\n",
    "                        self.print(\"Seen so far: %d samples\" % ((step + 1) * self.batch_size), verbose)\n",
    "\n",
    "                # Display metrics at the end of each epoch.\n",
    "                train_acc = self.train_acc_metric.result()\n",
    "                self.print(\"\\nTraining acc over epoch: %.4f\" % (float(train_acc),), verbose)\n",
    "\n",
    "                # Reset training metrics at the end of each epoch\n",
    "                self.train_acc_metric.reset_states()\n",
    "\n",
    "                # Run an evaluation loop at the end of each epoch.\n",
    "                for x_batch_test, y_batch_test in test_data:\n",
    "                    self.test_step(x_batch_test, y_batch_test)\n",
    "                eval_acc = self.val_acc_metric.result()\n",
    "                self.val_acc_metric.reset_states()\n",
    "                self.print(\"\\nTesting acc: %.4f\" % (float(eval_acc),), verbose)\n",
    "                self.print(\"Time taken: %.2fs\" % (time.time() - start_time), verbose)\n",
    "                \n",
    "                self.SavePretrainedModel.on_epoch_end(epoch)\n",
    "                \n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Finished Training Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EncodedSeq = List[int]\n",
    "\n",
    "def normalize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    # remove timestamps and double spaces\n",
    "    regexp = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:               # Match all enclosed\n",
    "        \\d{4}-\\d{2}-\\d{2} # YYYY-MM-DD\n",
    "        [\\sT]             # Accept either a space or T\n",
    "        \\d{2}:\\d{2}:\\d{2} # HH:MM:SS\n",
    "        ([.,]\\d{3}|\\s)    # Accept either a space or milliseconds\n",
    "        )                 # End timestamp match\n",
    "        | (?:\\s{2,})      # Remove double spaces   \n",
    "        | [^a-zA-Z\\d:]    # Clean non-alphanumeric characters\n",
    "        \"\"\", re.X)        # re.X enables comments and whitespace\n",
    "\n",
    "    c_logs = deepcopy(logs)\n",
    "    c_logs.loc[:, 'log'].replace(\n",
    "        to_replace=regexp, \n",
    "        value=' ', \n",
    "        regex=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return c_logs\n",
    "\n",
    "\n",
    "def extract_unique_labels(logs: pd.DataFrame) -> dict:\n",
    "    # -- Labels -- #\n",
    "    label_unique = logs['label'].unique()\n",
    "    binary_labels = LabelEncoder().fit_transform(label_unique)\n",
    "\n",
    "    log_labels = {}\n",
    "    # TODO: This seems a bit messy, could it be cleaned up? \n",
    "    for idx, label in enumerate(label_unique):\n",
    "        log_labels.update({\n",
    "            label: binary_labels[idx]\n",
    "        })\n",
    "    return log_labels\n",
    "\n",
    "\n",
    "def create_sentence_pairing(examples):\n",
    "    first_seqs = []\n",
    "    nsp_labels = []\n",
    "\n",
    "    examples[\"log\"] = [\n",
    "        line for line in examples[\"log\"] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    \n",
    "    log_list = list(examples['log'])\n",
    "    for idx in range(len(log_list)):\n",
    "        first_value = log_list[idx]\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "\n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            second_value = log_list[rand_idx]\n",
    "\n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "\n",
    "        first_seqs.append((first_value, second_value))\n",
    "\n",
    "    return {\"log\": first_seqs, \"next_sentence_label\": nsp_labels}\n",
    "\n",
    "\n",
    "def generate_test_train_split(tokenized_datasets, test_size=.30):\n",
    "    # Train - Test Split\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        list(range(len(tokenized_datasets))), test_size=test_size\n",
    "    )\n",
    "\n",
    "    test_dataset = tokenized_datasets.select(test_indices)\n",
    "    train_dataset = tokenized_datasets.select(train_indices)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "class UnsupervisedLearningPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PreprocessingPipelineConfig, \n",
    "        epochs: int = 3, \n",
    "        batch_size: int = 50, \n",
    "        seq_length: int = 200\n",
    "    ) -> None:\n",
    "\n",
    "        # Logs\n",
    "        self.normalized_logs = None\n",
    "        self.log_labels = None\n",
    "        self.logs_with_phrases = list()\n",
    "        \n",
    "        # Tokenizers\n",
    "        self.bert_tokenizer = PrimeTokenizer(seq_length)\n",
    "        self.fast_tokenizer = None\n",
    "        \n",
    "        # Dataset for Training/Evaluation\n",
    "        self.data_collator = None\n",
    "        self.tf_train_dataset = None\n",
    "        self.tf_test_dataset = None\n",
    "        self.token_logs = None\n",
    "                \n",
    "        # Models\n",
    "        self.PhoBert = None\n",
    "        \n",
    "        self.pm = PhraserModel(config.PhraserModelConfig, \n",
    "                               config.PreprocessingGlobalConfig)\n",
    "        self.tc = TextClustering(config.TextClusteringConfig,\n",
    "                                 config.PreprocessingGlobalConfig)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.max_seq_len = seq_length\n",
    "        self.n_logs = 0\n",
    "        self.n_iter = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_per_epoch = None\n",
    "        self.logs_as_templates = None\n",
    "        \n",
    "\n",
    "    def initialize_fast_tokenizer(self):\n",
    "        tokenizer_obj = self.bert_tokenizer.get_tokenizer()\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj)\n",
    "        fast_tokenizer.model_max_length = self.max_seq_len\n",
    "        fast_tokenizer.unk_token = \"[UNK]\"\n",
    "        fast_tokenizer.sep_token = \"[SEP]\"\n",
    "        fast_tokenizer.pad_token = \"[PAD]\"\n",
    "        fast_tokenizer.cls_token = \"[CLS]\"\n",
    "        fast_tokenizer.mask_token = \"[MASK]\"\n",
    "        self.fast_tokenizer = fast_tokenizer\n",
    "        return fast_tokenizer\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_data_signatures(train_dataset, test_dataset):\n",
    "        # Train Signatures\n",
    "        train_signature = {\n",
    "            feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "            for feature in train_dataset.features\n",
    "            if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "        }\n",
    "        train_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "        train_signature = (train_signature, {\"labels\": train_signature[\"labels\"], \"next_sentence_label\": train_signature[\"next_sentence_label\"]})\n",
    "        \n",
    "        # Test Signatures\n",
    "        test_signature = {\n",
    "            feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "            for feature in test_dataset.features\n",
    "            if feature != \"special_tokens_mask\" and feature != \"next_sentence_label\"\n",
    "        }\n",
    "        test_signature[\"next_sentence_label\"] = tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        test_signature[\"labels\"] = test_signature[\"input_ids\"]\n",
    "        test_signature = (test_signature, {\"labels\": test_signature[\"labels\"], \"next_sentence_label\": test_signature[\"next_sentence_label\"]})\n",
    "        \n",
    "        return train_signature, test_signature\n",
    "    \n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        # Remove empty lines\n",
    "        return self.fast_tokenizer(\n",
    "            examples[\"log\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_pre_training_data(self):\n",
    "        fast_tokenizer = self.initialize_fast_tokenizer()\n",
    "        \n",
    "        dt = self.normalized_logs.drop([\"label\", \"container_name\", \"timestamp\"], axis=1)\n",
    "        data = Dataset.from_pandas(dt)\n",
    "        \n",
    "        tokenized_datasets = data.map(\n",
    "            create_sentence_pairing,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"log\"],\n",
    "            desc=\"Creating sentence pairings for NSP Head\"\n",
    "        )\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"log\"],\n",
    "            desc=\"Running tokenizer on dataset line_by_line\"\n",
    "        )\n",
    "        \n",
    "        train_dataset, test_dataset = generate_test_train_split(tokenized_datasets)\n",
    "        train_signature, test_signature = self.generate_data_signatures(train_dataset, test_dataset)\n",
    "        \n",
    "        self.batches_per_epoch = len(train_dataset) // self.batch_size\n",
    "        \n",
    "        data_collator = TFDataCollatorForLanguageModeling(\n",
    "            tokenizer=self.fast_tokenizer,\n",
    "            padding_length=self.max_seq_len,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        \n",
    "        tokenized_generator = partial(data_collator, train_dataset, fast_tokenizer)\n",
    "        test_generator = partial(data_collator, test_dataset, fast_tokenizer)\n",
    "        \n",
    "        self.tf_train_dataset = (\n",
    "            tf.data.Dataset.from_generator(tokenized_generator, output_signature=train_signature)\n",
    "            .batch(batch_size=self.batch_size, drop_remainder=True)\n",
    "            .repeat(int(5))\n",
    "        )\n",
    "        \n",
    "        self.tf_test_dataset = (\n",
    "            tf.data.Dataset.from_generator(test_generator, output_signature=test_signature)\n",
    "            .batch(batch_size=self.batch_size, drop_remainder=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_bert_tokenizer(self, load_model=False):\n",
    "        if load_model:\n",
    "            self.bert_tokenizer.load()\n",
    "        else:\n",
    "            self.bert_tokenizer.train(self.logs_as_templates)\n",
    "            \n",
    "            \n",
    "    def pre_train_bert(self, sp):\n",
    "        # PhoBert Model instantiation\n",
    "        self.PhoBert = PhoBert(\n",
    "            self.bert_tokenizer.get_vocab_size(),\n",
    "            self.batch_size,\n",
    "            self.epochs,\n",
    "            self.batches_per_epoch,\n",
    "            len(self.fast_tokenizer)\n",
    "        )\n",
    "        \n",
    "        # Prefetch Data\n",
    "        self.tf_train_dataset = self.tf_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        self.tf_test_dataset = self.tf_test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Start PreTraining\n",
    "        sp.text = \"PreTraining BERT\"\n",
    "        self.PhoBert(self.tf_train_dataset, self.tf_test_dataset, True)\n",
    "            \n",
    "    \n",
    "    def run_bert(self, batch):\n",
    "        return self.BERT(batch,\n",
    "                         training=False,\n",
    "                         output_attentions=True)\n",
    "    \n",
    "\n",
    "    def fit(self, logs: pd.DataFrame):\n",
    "        assert len(logs.index) > 0, 'process received an empty dataframe!'\n",
    "        \n",
    "        with yaspin(text=\"Normalizing Logs\", color='green') as sp:\n",
    "            self.normalized_logs = normalize_logs(logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log normalization')\n",
    "            \n",
    "            sp.text = \"Extracting phrases\"\n",
    "            self.logs_with_phrases = self.pm(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed phrase extraction')\n",
    "            \n",
    "            sp.text = \"Converting to log templates\"\n",
    "            self.logs_as_templates = np.array(self.tc(self.logs_with_phrases))\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log template conversion')\n",
    "        \n",
    "            sp.text = \"Extracting Unique Labels\"\n",
    "            self.log_labels = extract_unique_labels(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed extracting unique labels')\n",
    "\n",
    "            sp.text = \"Training Tokenizer\"\n",
    "            self.train_bert_tokenizer()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed training of custom tokenizer')\n",
    "                        \n",
    "            sp.text = \"Processing training dataset\"\n",
    "            self.get_pre_training_data()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed processing training dataset')\n",
    "            \n",
    "#             sp.text = \"Pretraining BERT\"\n",
    "            self.pre_train_bert(sp)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed BERT pretraining')\n",
    "            \n",
    "        \n",
    "    def transform(self, batch: pd.DataFrame):\n",
    "        x = normalize_logs(batch)\n",
    "        x = self.pm(x, False)\n",
    "#         x = list(self.tc(x, False))\n",
    "#         x = self.serve_batch(x)\n",
    "        return self.run_bert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V Pipeline Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_path = SOURCE + '/assets/notebooks/PreprocessingConfig.yaml'\n",
    "preprocessing_config = PreprocessingPipelineConfig()\n",
    "preprocessing_config.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19 14:09:50,941 INFO | Starting Drain3 template miner\n",
      "2021-07-19 14:09:50,943 INFO | Loading configuration from drain3.ini\n",
      "\u001b[K\u001b[32m✔ Completed log normalization\u001b[0m \n",
      "2021-07-19 14:09:51,087 INFO | exporting phrases from Phrases<0 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n",
      "2021-07-19 14:09:51,087 INFO | FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<0 phrases, min_count=5, threshold=7> from Phrases<0 vocab, min_count=5, threshold=7, max_vocab_size=40000000> in 0.00s', 'datetime': '2021-07-19T14:09:51.087732', 'gensim': '4.0.1', 'python': '3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \\n[GCC 9.3.0]', 'platform': 'Linux-5.8.0-55-generic-x86_64-with-glibc2.10', 'event': 'created'}\n",
      "\u001b[K\u001b[32m✔ Completed phrase extraction\u001b[0m \n",
      "\u001b[K\u001b[32m✔ Completed log template conversion\u001b[0m \n",
      "\u001b[K\u001b[32m✔ Completed extracting unique labels\u001b[0m \n",
      "\n",
      "\n",
      "\n",
      "\u001b[K\u001b[32m✔ Completed training of custom tokenizer\u001b[0m \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b39d56e272249ecbe9de88d25de89e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating sentence pairings for NSP Head:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51be8d64eff44af3b8fb58f8762506e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset line_by_line:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[32m✔ Completed processing training dataset\u001b[0m \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (2508, 512) is not compatible with supplied shape (2503, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_929/2081048162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# --- SUBWORD TOKENIZER --\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mw2vp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnsupervisedLearningPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtraining_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2vp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_929/2569773260.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;31m#             sp.text = \"Pretraining BERT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_train_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'✔ Completed BERT pretraining'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_929/2569773260.py\u001b[0m in \u001b[0;36mpre_train_bert\u001b[0;34m(self, sp)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_train_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# PhoBert Model instantiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         self.PhoBert = PhoBert(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_929/448000291.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, batch_size, epochs, batches_per_epoch, token_len)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchkpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchkpt_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchkpt_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRestored from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchkpt_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m       raise errors_impl.NotFoundError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2146\u001b[0m     \"\"\"\n\u001b[1;32m   2147\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcheckpoint_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mgraph_view\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_view\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         options=options)\n\u001b[0;32m-> 1336\u001b[0;31m     base.CheckpointPosition(\n\u001b[0m\u001b[1;32m   1337\u001b[0m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0mcurrent_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisit_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       new_restore_ops, new_tensor_saveables, new_python_saveables = (\n\u001b[0;32m--> 964\u001b[0;31m           \u001b[0mcurrent_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrackable\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m           ._single_restoration_from_checkpoint_position(\n\u001b[1;32m    966\u001b[0m               \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_single_restoration_from_checkpoint_position\u001b[0;34m(self, checkpoint_position, visit_queue)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                                                []).append(child_position)\n\u001b[1;32m   1001\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mchild_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m           \u001b[0;31m# This object's correspondence is new, so dependencies need to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m           \u001b[0;31m# visited. Delay doing it so that we get a breadth-first dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mbind_object\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_create_or_restore_slot_variable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m           optimizer_object._create_or_restore_slot_variable(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    303\u001b[0m               slot_variable_position=CheckpointPosition(\n\u001b[1;32m    304\u001b[0m                   \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_create_or_restore_slot_variable\u001b[0;34m(self, slot_variable_position, slot_name, variable)\u001b[0m\n\u001b[1;32m   1312\u001b[0m       initializer = trackable.CheckpointInitialValueCallable(\n\u001b[1;32m   1313\u001b[0m           checkpoint_position=slot_variable_position)\n\u001b[0;32m-> 1314\u001b[0;31m       slot_variable = self.add_slot(\n\u001b[0m\u001b[1;32m   1315\u001b[0m           \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36madd_slot\u001b[0;34m(self, var, slot_name, initializer)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         weight = tf_variables.Variable(\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%s/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     return previous_getter(\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    235\u001b[0m                         shape=None):\n\u001b[1;32m    236\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2654\u001b[0;31m   return resource_variable_ops.ResourceVariable(\n\u001b[0m\u001b[1;32m   2655\u001b[0m       \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1572\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m       self._init_from_args(\n\u001b[0m\u001b[1;32m   1575\u001b[0m           \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1710\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m               \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInitialValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_initialize_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, shard_info)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# will get passed in by a functool.partial_wrapper in places like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# base_layer_utils.py's make_variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     return CheckpointInitialValue(\n\u001b[0m\u001b[1;32m     82\u001b[0m         self._checkpoint_position, shape, shard_info=shard_info)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_position, shape, shard_info)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;31m# We need to set the static shape information on the initializer if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;31m# possible so we don't get a variable with an unknown shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   1216\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m           (self.shape, shape))\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (2508, 512) is not compatible with supplied shape (2503, 512)"
     ]
    }
   ],
   "source": [
    "# -- Unsupervised Learning Pipeline -- #\n",
    "\n",
    "'''\n",
    "Input: pd.DataFrame with batch_size number of rows \n",
    "Seq: \n",
    "    Normalize \n",
    "    Phraser\n",
    "    Clustering\n",
    "    Extract Unique Layers\n",
    "    BERT\n",
    "Returns: transformers.TFBertForPreTrainingOutput\n",
    "'''\n",
    "\n",
    "# --- SUBWORD TOKENIZER --\n",
    "w2vp = UnsupervisedLearningPipeline(preprocessing_config, epochs=2, batch_size=10, seq_length=300)\n",
    "training_outputs = w2vp.fit(dataset[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data['log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = test_data.iloc[0]['log']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens = w2vp.bert_tokenizer.text_to_sequence(sentence).tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert = deepcopy(w2vp.BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "copy_bert.get_output_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2vp.BERT.save(SOURCE + '/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITSJA5hWASDn",
    "0JtgL-iQkqj2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "LongRunTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
