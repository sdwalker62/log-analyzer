{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -- Base -- #\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import yaml\n",
    "import csv\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict\n",
    ")\n",
    "from yaspin import yaspin\n",
    "\n",
    "# -- Tokenizer -- #\n",
    "import tokenizers\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "# -- PreTrained BERT -- #\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# -- Metrics -- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- Misc Models -- #\n",
    "import drain3\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -- Custom -- #\n",
    "from libs.transformers.src.transformers.models.bert.modeling_tf_bert import TFBertForPreTraining\n",
    "from libs.transformers.src.transformers.models.bert.configuration_bert import BertConfig\n",
    "from libs.transformers.src.transformers.modeling_tf_utils import shape_list\n",
    "from libs.transformers.src.transformers.data.data_tf_collator import TFDataCollatorForLanguageModeling\n",
    "\n",
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# needed to access local files\n",
    "cd .."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_sqlite_to_csv(inputFolder, ext, tableName):\n",
    "    \"\"\" inputFolder - Folder where sqlite files are located. \n",
    "        ext - Extension of your sqlite file (eg. db, sqlite, sqlite3 etc.)\n",
    "        tableName - table name from which you want to select the data.\n",
    "    \"\"\"\n",
    "    csvWriter = csv.writer(open(inputFolder+'/output.csv', 'w', newline=''))\n",
    "    for file1 in os.listdir(inputFolder):\n",
    "        if file1.endswith('.'+ext):\n",
    "            conn = sql.connect(inputFolder+'/'+file1)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT * FROM \"+tableName)\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                csvWriter.writerow(row)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extensions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environmental Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SOURCE = '/home/' + os.environ['USER']\n",
    "CONTAINER = 'core.soaesb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s | %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Database Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def database_builder(path: str) -> pd.DataFrame():\n",
    "    logger.info('Building DataFrame ...')\n",
    "    (_, _, files) = next(os.walk(path))\n",
    "    sql_query = 'SELECT * FROM logs'\n",
    "    data = []\n",
    "    for f in files:\n",
    "        if '.db' in f:\n",
    "            conn = create_connection(path + f)\n",
    "            d = pd.read_sql_query(sql_query, conn)\n",
    "            data.append(d)\n",
    "    logger.info('...complete!')\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def create_connection(path: str) -> sql.Connection:\n",
    "    \"\"\"\n",
    "    Creates a database connection\n",
    "    :param path: str\n",
    "        path to database object\n",
    "    :return sql.Connection\n",
    "        a connection to the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sql.connect(path)\n",
    "        logger.info('Connected to database ' + path)\n",
    "        return conn\n",
    "    except sql.Error as e:\n",
    "        logger.warning(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataset Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = database_builder(SOURCE + '/data/')\n",
    "container_dataset = dataset[dataset['container_name'] == CONTAINER]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline Objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_attributes(self, config: dict):\n",
    "    try:\n",
    "        config = config[self.__class__.__name__]\n",
    "    except Exception as e:\n",
    "        logger.warning(e)\n",
    "        logger.warning('No configuration found for ' +\n",
    "                       self.__class__.__name__)\n",
    "\n",
    "    for attr in config.keys():\n",
    "        setattr(self, attr, config[attr])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingGlobalConfig:\n",
    "    embed_size: int = 512\n",
    "    max_vocab_size: int = 2000\n",
    "    buffer_size: int = 10000\n",
    "    global_training: bool = True\n",
    "    path: str = '/results/'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PhraserModelConfig:\n",
    "    min_count: int = 5\n",
    "    threshold: float = 7\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'phrase_model.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClusteringConfig:\n",
    "    load_model: bool = True\n",
    "    save_model: bool = False\n",
    "    training: bool = True\n",
    "    model_name: str = 'template_miner.joblib'\n",
    "\n",
    "    def load(self, config):\n",
    "        set_attributes(self, config)\n",
    "\n",
    "\n",
    "class PreprocessingPipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.PreprocessingGlobalConfig = PreprocessingGlobalConfig()\n",
    "        self.PhraserModelConfig = PhraserModelConfig()\n",
    "        self.TextClusteringConfig = TextClusteringConfig()\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                preprocessing_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except FileNotFoundError as e:\n",
    "            logger.warning(e)\n",
    "            return None\n",
    "\n",
    "        self.PreprocessingGlobalConfig.load(preprocessing_config)\n",
    "        self.PhraserModelConfig.load(preprocessing_config)\n",
    "        self.TextClusteringConfig.load(preprocessing_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PrimeTokenizer:\n",
    "    def __init__(self, max_seq_length: int):\n",
    "        self.prime_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.prime_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "        self.prime_tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.prime_tokenizer.decoder = decoders.WordPiece()\n",
    "        self.prime_tokenizer.enable_padding(length=max_seq_length)\n",
    "        self.prime_tokenizer.enable_truncation(max_seq_length)\n",
    "\n",
    "        self.prime_tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.trainer = WordPieceTrainer(\n",
    "            vocab_size=153411,\n",
    "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "    def text_to_sequence(self, input_) -> List[tokenizers.Encoding]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.encode_batch(input_)\n",
    "        return self.prime_tokenizer.encode(input_)\n",
    "\n",
    "    def sequence_to_text(self, input_) -> List[str]:\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.decode_batch(batch)\n",
    "        return self.prime_tokenizer.decode(input_)\n",
    "\n",
    "    def train(self, data):\n",
    "        log_itr = iter(data)\n",
    "        self.prime_tokenizer.train_from_iterator(log_itr, self.trainer)\n",
    "        self.save()\n",
    "\n",
    "    def get_tokenizer(self) -> Tokenizer:\n",
    "        return self.prime_tokenizer\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.prime_tokenizer.get_vocab()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.prime_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def save(self):\n",
    "        self.prime_tokenizer.save(SOURCE + \"/results/prime_tokenizer.json\")\n",
    "        \n",
    "    def load(self):\n",
    "        self.prime_tokenizer = Tokenizer.from_file(SOURCE + \"/results/prime_tokenizer.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generic Save Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "#     if not os.path.exists(path):\n",
    "#         return\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "#     elif os.path.isdir(path):\n",
    "#         shutil.rmtree(path)\n",
    "#         return\n",
    "\n",
    "    joblib.dump(model, path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PhraseCaptureLayer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PhraserModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: PhraserModelConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(PhraserModel, self).__init__()\n",
    "        self.min_count = config.min_count\n",
    "        self.threshold = config.threshold\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model:\n",
    "            self.phrase_model = joblib.load(SOURCE +\n",
    "                                            self.path +\n",
    "                                            self.model_name)\n",
    "        else:\n",
    "            self.phrase_model = Phrases(min_count=self.min_count,\n",
    "                                        threshold=self.threshold)\n",
    "\n",
    "    def __call__(self, corpus: pd.DataFrame, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        def reorganize_return(corpus_with_phrases):\n",
    "            log_list = []\n",
    "            for tokenized_log in corpus_with_phrases:\n",
    "                log_list.append(' '.join(tokenized_log))\n",
    "            return log_list\n",
    "\n",
    "        split_corpus = [log.split(' ') for log in corpus['log']]\n",
    "\n",
    "        corpus_with_phrases = None\n",
    "        if not training:\n",
    "            frozen_model = self.phrase_model.freeze()\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "        else:\n",
    "            self.phrase_model.add_vocab(split_corpus)\n",
    "\n",
    "            if self.save_model:\n",
    "                save_model(self.phrase_model, SOURCE + self.path + self.model_name)\n",
    "\n",
    "            corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "            \n",
    "        return reorganize_return(corpus_with_phrases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TextClusteringLayer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TextClustering:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: TextClusteringConfig,\n",
    "                 global_config: PreprocessingGlobalConfig):\n",
    "\n",
    "        super(TextClustering, self).__init__()\n",
    "        self.load_model = config.load_model\n",
    "        self.save_model = config.save_model\n",
    "        self.path = global_config.path\n",
    "        self.model_name = config.model_name\n",
    "        self.training = config.training\n",
    "\n",
    "        if self.load_model is True:\n",
    "            self.template_miner = joblib.load(SOURCE +\n",
    "                                              self.path +\n",
    "                                              self.model_name)\n",
    "        else:\n",
    "            self.template_miner = drain3.TemplateMiner()\n",
    "\n",
    "    def __call__(self, corpus: list, training=None) -> list:\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "            \n",
    "        if training:\n",
    "            for log in corpus:\n",
    "                self.template_miner.add_log_message(log)\n",
    "            if self.save_model:\n",
    "                save_model(self.template_miner,\n",
    "                           SOURCE + self.path + self.model_name)\n",
    "\n",
    "            for idx, log in enumerate(corpus):\n",
    "                template = self.template_miner.match(log).get_template()\n",
    "                corpus[idx] = template\n",
    "\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster) for cluster in corpus]\n",
    "        else:\n",
    "#             log_list = self.get_unique_templates()\n",
    "#             print(f'Length of the log list: {len(log_list)}')\n",
    "#             return log_list\n",
    "            log_list = []\n",
    "            log_set = set()\n",
    "            for log in corpus:\n",
    "                match_cluster = self.template_miner.match(log)\n",
    "                if match_cluster is None:\n",
    "                    match_cluster = self.template_miner.add_log_message(log)['template_mined']\n",
    "                    log_set.add(match_cluster)\n",
    "                else:\n",
    "                    log_set.add(match_cluster.get_template())\n",
    "        \n",
    "#             l = [re.sub(pattern=r' +',\n",
    "#                            repl=' ',\n",
    "#                            string=cluster) for cluster in log_list]\n",
    "            return list(log_set)\n",
    "        \n",
    "    def get_unique_templates(self) -> list:\n",
    "        template_list = []\n",
    "        for cluster in self.template_miner.drain.clusters:\n",
    "            template_list.append(cluster.get_template())\n",
    "        return [re.sub(pattern=r' +',\n",
    "                       repl=' ',\n",
    "                       string=cluster) for cluster in template_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_all_batches(n_iter, log_labels, batch_size):\n",
    "    batches = []\n",
    "\n",
    "    for idx in range(n_iter + 1):\n",
    "        log_batch, labels = process_batch(dataset, idx, log_labels, batch_size)\n",
    "\n",
    "        batches.append((log_batch, labels))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def process_batch(dataset: pd.DataFrame,\n",
    "                  idx: int,\n",
    "                  labels: dict,\n",
    "                  batch_size: int) -> tuple:\n",
    "    start_window = idx * batch_size\n",
    "    end_window = (idx + 1) * batch_size\n",
    "    batched_data = dataset.iloc[start_window:end_window]\n",
    "    encoded_batch = prime_tokenizer.text_to_sequence(batched_data['log'].to_list())\n",
    "    id_batch = [log.ids for log in encoded_batch]\n",
    "#     y_batch = labels[batched_data['label']]\n",
    "    y_batch = [labels[idx] for idx in batched_data['label']]\n",
    "\n",
    "    tf_idf = tf.convert_to_tensor(id_batch, dtype=tf.float32)\n",
    "    y_idf  = tf.convert_to_tensor(y_batch, dtype=tf.float32)\n",
    "    \n",
    "    return tf_idf, y_idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unsupervised Learning Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EncodedSeq = list[int]\n",
    "\n",
    "\n",
    "def normalize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    # remove timestamps and double spaces\n",
    "    regexp = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:               # Match all enclosed\n",
    "        \\d{4}-\\d{2}-\\d{2} # YYYY-MM-DD\n",
    "        [\\sT]             # Accept either a space or T\n",
    "        \\d{2}:\\d{2}:\\d{2} # HH:MM:SS\n",
    "        ([.,]\\d{3}|\\s)    # Accept either a space or milliseconds\n",
    "        )                 # End timestamp match\n",
    "        | (?:\\s{2,})      # Remove double spaces   \n",
    "        | [^a-zA-Z\\d:]    # Clean non-alphanumeric characters\n",
    "        \"\"\", re.X)        # re.X enables comments and whitespace\n",
    "\n",
    "    c_logs = deepcopy(logs)\n",
    "    c_logs.loc[:, 'log'].replace(\n",
    "        to_replace=regexp, \n",
    "        value=' ', \n",
    "        regex=True,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    return c_logs\n",
    "\n",
    "\n",
    "def batch_to_tensor(batch) -> tf.Tensor:\n",
    "    enc_list = []\n",
    "    for enc_obj in batch:\n",
    "        enc_tensor = tf.convert_to_tensor(enc_obj, dtype=tf.int32)\n",
    "        enc_list.append(enc_tensor)\n",
    "    return tf.stack(enc_list, name='tensor_batch')\n",
    "\n",
    "\n",
    "def serve_batch(batch, tokenizer):\n",
    "    log_batch = tokenizer.text_to_sequence(batch)\n",
    "    batch_ids = [log.ids for log in log_batch]\n",
    "    return batch_to_tensor(batch_ids)\n",
    "\n",
    "\n",
    "def extract_unique_labels(logs: pd.DataFrame) -> dict:\n",
    "    # -- Labels -- #\n",
    "    label_unique = logs['label'].unique()\n",
    "    binary_labels = LabelEncoder().fit_transform(label_unique)\n",
    "\n",
    "    log_labels = {}\n",
    "    # TODO: This seems a bit messy, could it be cleaned up? \n",
    "    for idx, label in enumerate(label_unique):\n",
    "        log_labels.update({\n",
    "            label: binary_labels[idx]\n",
    "        })\n",
    "    return log_labels\n",
    "\n",
    "\n",
    "def create_sentence_pairing(\n",
    "    logs: pd.DataFrame, \n",
    "    n_iter: int, \n",
    "    batch_size: int\n",
    ") -> tuple[list[EncodedSeq], list[EncodedSeq], list[int]]:\n",
    "\n",
    "    first_seqs = [], second_seqs = [], nsp_labels = []\n",
    "    log_list = logs['log'].to_list()\n",
    "    \n",
    "    for idx in range(len(log_list)):\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "            \n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            # TODO: can rand_idx == idx? or should we add a check here?\n",
    "            second_value = log_list[rand_idx]\n",
    "            \n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "            \n",
    "        first_seqs.append(log_list[idx])\n",
    "        second_seqs.append(second_value)\n",
    "        \n",
    "    nsp_labels = tf.reshape(\n",
    "        tf.constant(nsp_labels), \n",
    "        (n_iter, batch_size),\n",
    "        'nsp_labels'\n",
    "    )\n",
    "        \n",
    "    return first_seqs, second_seqs, nsp_labels\n",
    "\n",
    "\n",
    "def tokenize_data(tokenizer, groupings):\n",
    "    # Remove empty lines\n",
    "    return tokenizer(\n",
    "        groupings[0],\n",
    "        groupings[1],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_data(encodings, tokenizer, max_seq_len, batch_size):\n",
    "    # Assure data is non-ragged and are of type tf.Tensor\n",
    "    encodings['input_ids'] = tf.ragged.constant(encodings['input_ids']).to_tensor(0)\n",
    "    encodings['token_type_ids'] = tf.ragged.constant(encodings['token_type_ids']).to_tensor(0)\n",
    "    encodings['attention_mask'] = tf.ragged.constant(encodings['attention_mask']).to_tensor(0)\n",
    "\n",
    "    batch_encodings = tf.data.Dataset.from_tensor_slices(encodings.data)\n",
    "\n",
    "    data_collator = TFDataCollatorForLanguageModeling(\n",
    "        tokenizer,\n",
    "        padding_length=max_seq_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return data_collator(batch_encodings)\n",
    "\n",
    "\n",
    "class UnsupervisedLearningPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PreprocessingPipelineConfig, \n",
    "        epochs: int = 3, \n",
    "        batch_size: int = 50, \n",
    "        seq_length: int = 200\n",
    "    ) -> None:\n",
    "\n",
    "        self.normalized_logs = None\n",
    "        self.log_labels = None\n",
    "        self.bert_tokenizer = PrimeTokenizer(seq_length)\n",
    "        self.fast_tokenizer = None\n",
    "        self.train_dataset = None\n",
    "        self.token_logs = None\n",
    "        self.bert_config = None\n",
    "        self.BERT = None\n",
    "        self.max_seq_len = seq_length\n",
    "        self.n_logs = 0\n",
    "        self.n_iter = 0\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.logs_with_phrases = list()\n",
    "        self.logs_as_templates = None\n",
    "        self.data_collator = None\n",
    "        \n",
    "        self.pm = PhraserModel(config.PhraserModelConfig, \n",
    "                               config.PreprocessingGlobalConfig)\n",
    "        \n",
    "        self.tc = TextClustering(config.TextClusteringConfig,\n",
    "                                 config.PreprocessingGlobalConfig)\n",
    "        \n",
    "\n",
    "    def initialize_fast_tokenizer(self):\n",
    "        tokenizer_obj = self.bert_tokenizer.get_tokenizer()\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer_obj)\n",
    "        fast_tokenizer.model_max_length = self.max_seq_len\n",
    "        fast_tokenizer.unk_token = \"[UNK]\"\n",
    "        fast_tokenizer.sep_token = \"[SEP]\"\n",
    "        fast_tokenizer.pad_token = \"[PAD]\"\n",
    "        fast_tokenizer.cls_token = \"[CLS]\"\n",
    "        fast_tokenizer.mask_token = \"[MASK]\"\n",
    "        self.fast_tokenizer = fast_tokenizer\n",
    "        return fast_tokenizer\n",
    "\n",
    "\n",
    "    def get_pre_training_data(self):\n",
    "        labels = self.normalized_logs['label'].to_list()\n",
    "        log_labels = [self.log_labels[l] for l in labels]\n",
    "\n",
    "        fast_tokenizer = self.initialize_fast_tokenizer()\n",
    "        \n",
    "        first_sequences, pair_sequences, nsp_labels = create_sentence_pairing(\n",
    "            self.normalized_logs,\n",
    "            self.n_iter,\n",
    "            self.batch_size\n",
    "        )\n",
    "        \n",
    "        encodings = fast_tokenizer(\n",
    "            first_sequences,\n",
    "            pair_sequences,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "\n",
    "#         tokenized_datasets = data.map(\n",
    "#             tokenize_function,\n",
    "#             batched=True,\n",
    "#             num_proc=1,\n",
    "#             remove_columns=[\"log\"],\n",
    "#             desc=\"Running tokenizer on dataset line_by_line\"\n",
    "#         )\n",
    "                \n",
    "        encoded_objs = collate_data(\n",
    "            encodings,\n",
    "            self.fast_tokenizer,\n",
    "            self.batch_size,\n",
    "            self.max_seq_len\n",
    "        )\n",
    "        \n",
    "        pre_training_data = dict()\n",
    "        target_values = dict()\n",
    "        for key in list(encoded_objs)[0].keys():\n",
    "            temp_tensor = tf.constant([x[key].numpy() for x in encoded_objs])\n",
    "            if key == \"labels\":\n",
    "                target_values[key] = temp_tensor\n",
    "                continue\n",
    "            pre_training_data[key] = temp_tensor\n",
    "        \n",
    "#         pre_training_data[\"next_sentence_label\"] = nsp_labels\n",
    "        target_values[\"next_sentence_label\"] = nsp_labels\n",
    "        \n",
    "#         log_labels = tf.reshape(tf.constant(log_labels), (self.n_iter, self.batch_size))\n",
    "        \n",
    "        pre_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            pre_training_data,\n",
    "            target_values\n",
    "        ))\n",
    "    \n",
    "        pre_train_dataset = pre_train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        return pre_train_dataset\n",
    "\n",
    "\n",
    "    def train_bert_tokenizer(self, load_model=False):\n",
    "        if load_model:\n",
    "            self.bert_tokenizer.load()\n",
    "        else:\n",
    "            self.bert_tokenizer.train(self.logs_as_templates)\n",
    "\n",
    "\n",
    "    def serve_batch_w_idx(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        \n",
    "        log_batch = self.logs_as_templates[start_idx:end_idx].tolist()\n",
    "        batch = self.bert_tokenizer.text_to_sequence(log_batch)\n",
    "        batch_ids = [log.ids for log in batch]\n",
    "        return batch_to_tensor(batch_ids)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def custom_compute_loss(self, labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        # make sure only labels that are not equal to -100\n",
    "        # are taken into account as loss\n",
    "#         print(f\"HEY: {logits.name}\")\n",
    "        \n",
    "#         if logits.shape == ():\n",
    "#             return 0\n",
    "        \n",
    "#         if \"seq_relationship\" in logits.name:\n",
    "#             next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels, shape=(-1,)), -100)\n",
    "#             next_sentence_reduced_logits = tf.boolean_mask(\n",
    "#                 tensor=tf.reshape(tensor=logits, shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "#             )\n",
    "#             next_sentence_label = tf.boolean_mask(\n",
    "#                 tensor=tf.reshape(tensor=labels, shape=(-1,)), mask=next_sentence_active_loss\n",
    "#             )\n",
    "#             next_sentence_loss = tf.cast(loss_fn(y_true=next_sentence_label, y_pred=next_sentence_reduced_logits), tf.float32)\n",
    "#             print(next_sentence_loss)\n",
    "#             return next_sentence_loss\n",
    "            \n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels, shape=(-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits, shape=(-1, shape_list(logits)[2])),\n",
    "            mask=masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels, shape=(-1,)), mask=masked_lm_active_loss\n",
    "        )\n",
    "#         next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "#         next_sentence_reduced_logits = tf.boolean_mask(\n",
    "#             tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "#         )\n",
    "#         next_sentence_label = tf.boolean_mask(\n",
    "#             tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "#         )\n",
    "        masked_lm_loss = loss_fn(y_true=masked_lm_labels, y_pred=masked_lm_reduced_logits)\n",
    "        #next_sentence_loss = tf.cast(loss_fn(y_true=next_sentence_label, y_pred=next_sentence_reduced_logits), tf.float32)\n",
    "    #         masked_lm_loss = tf.reshape(tensor=masked_lm_loss, shape=(-1, shape_list(next_sentence_loss)[0]))\n",
    "    #         masked_lm_loss = tf.reduce_mean(input_tensor=masked_lm_loss, axis=0)\n",
    "\n",
    "        print(masked_lm_loss)\n",
    "        return masked_lm_loss #+ next_sentence_loss\n",
    "\n",
    "    # Given a callable model, inputs, outputs, and a learning rate...\n",
    "    def backprop(self, current_loss, learning_rate):\n",
    "\n",
    "      with tf.GradientTape() as t:\n",
    "          # Use GradientTape to calculate the gradients with respect to W and b\n",
    "          dw, db = t.gradient(current_loss, [self.BERT.bert.w, self.BERT.b])\n",
    "\n",
    "          # Subtract the gradient scaled by the learning rate\n",
    "          self.BERT.w.assign_sub(learning_rate * dw)\n",
    "          self.BERT.b.assign_sub(learning_rate * db)\n",
    "\n",
    "\n",
    "    def pre_train_bert(self):\n",
    "        self.bert_config = BertConfig(\n",
    "            vocab_size=self.bert_tokenizer.get_vocab_size(),\n",
    "            hidden_size=512,\n",
    "            num_hidden_layers=8,\n",
    "            num_attention_heads=8\n",
    "        )\n",
    "\n",
    "#         self.bert_config = CONFIG_MAPPING[\"bert\"]()\n",
    "        \n",
    "#         self.BERT = TFBertForPreTraining.from_config(self.bert_config)\n",
    "        self.BERT = TFBertForPreTraining(self.bert_config)\n",
    "        self.BERT.resize_token_embeddings(len(self.fast_tokenizer))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        self.BERT.compile(optimizer=optimizer, loss=self.BERT.compute_loss)\n",
    "        \n",
    "#         batch = list(self.train_dataset)[0]\n",
    "                \n",
    "        with yaspin(text=\"\", color='blue') as sp:\n",
    "            self.BERT.fit(self.train_dataset, epochs=5)\n",
    "#             for epoch in range(self.epochs):\n",
    "#             sp.text = f'Training BERT EPOCH {epoch}/{self.epochs}'\n",
    "#             for batch in self.train_dataset.as_numpy_iterator():\n",
    "# #                     self.BERT.fit(x={\"input_ids\": batch[0][\"input_ids\"], \n",
    "# #                                            \"attention_mask\": batch[0][\"attention_mask\"],\n",
    "# #                                            \"token_type_ids\": batch[0][\"token_type_ids\"]},\n",
    "# #                                          y={\"labels\": batch[0][\"labels\"],\n",
    "# #                                                \"next_sentence_label\": batch[0][\"next_sentence_label\"]},\n",
    "# #                                         epochs=5\n",
    "# #                                  )\n",
    "# #                     self.BERT.fit(batch[0], )\n",
    "#                 outputs = self.BERT(\n",
    "#                     input_ids=batch[0][\"input_ids\"],\n",
    "#                     attention_mask=batch[0][\"attention_mask\"],\n",
    "#                     token_type_ids=batch[0][\"token_type_ids\"],\n",
    "#                     labels=batch[0][\"labels\"],\n",
    "#                     next_sentence_label=batch[0][\"next_sentence_label\"],\n",
    "#                     training=True,\n",
    "#         #                         output_attentions=True,\n",
    "#         #                         output_hidden_states=True,\n",
    "#                     return_dict=True\n",
    "#                 )\n",
    "#                 self.BERT.fit(x=[batch[0][\"input_ids\"], batch[0][\"attention_mask\"], batch[0][\"token_type_ids\"]],\n",
    "#                                      y=outputs[1:3])\n",
    "    \n",
    "\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Finished Training Epochs')\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def run_bert(self, batch):\n",
    "        return self.BERT(batch,\n",
    "                         output_attentions=True)\n",
    "\n",
    "    def fit(self, logs: pd.DataFrame):\n",
    "        assert len(logs.index) > 0, 'process received an empty dataframe!'\n",
    "        \n",
    "        with yaspin(text=\"Normalizing Logs\", color='green') as sp:\n",
    "            self.normalized_logs = (self.normalize_logs(logs))\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log normalization')\n",
    "            \n",
    "            sp.text = \"Extracting phrases\"\n",
    "            self.logs_with_phrases = self.pm(self.normalized_logs)\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed phrase extraction')\n",
    "            \n",
    "            sp.text = \"Converting to log templates\"\n",
    "            self.logs_as_templates = np.array(self.tc(self.logs_with_phrases))\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed log template conversion')\n",
    "        \n",
    "            sp.text = \"Extracting Unique Labels\"\n",
    "            self.log_labels = self.extract_unique_labels()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed extracting unique labels')\n",
    "\n",
    "            sp.text = \"Training Tokenizer\"\n",
    "            self.train_bert_tokenizer()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed training of custom tokenizer')\n",
    "            \n",
    "            self.n_logs = len(self.normalized_logs['log'])\n",
    "            self.n_iter = self.n_logs // self.batch_size\n",
    "                        \n",
    "            sp.text = \"Processing training dataset\"\n",
    "            self.train_dataset = self.get_pre_training_data()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed processing training dataset')\n",
    "            \n",
    "            sp.text = \"Pretraining BERT\"\n",
    "            outputs = self.pre_train_bert()\n",
    "            sp.text = \"\"\n",
    "            sp.ok('✔ Completed BERT pretraining')\n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    def transform(self, batch: pd.DataFrame):\n",
    "        x = self.normalize_logs(batch)\n",
    "        x = self.pm(x, False)\n",
    "        x = list(self.tc(x, False))\n",
    "        x = self.serve_batch(x)\n",
    "        return self.run_bert(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import CONFIG_MAPPING, MODEL_FOR_MASKED_LM_MAPPING\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "print(\"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES))\n",
    "CONFIG_MAPPING[\"bert\"]()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list(w2vp.train_dataset)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## W2V Pipeline Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config_path = SOURCE + '/assets/notebooks/PreprocessingConfig.yaml'\n",
    "preprocessing_config = PreprocessingPipelineConfig()\n",
    "preprocessing_config.load(config_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vp.normalized_logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt = w2vp.normalized_logs.drop([\"label\", \"container_name\", \"timestamp\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(dt[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_generator(dataset, tokenizer, mlm_probability=0.15, pad_to_multiple_of=None):\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError(\"This tokenizer does not have a mask token which is necessary for masked language modeling. \")\n",
    "    # Trim off the last partial batch if present\n",
    "    sample_ordering = np.random.permutation(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        special_tokens_mask = example.pop(\"special_tokens_mask\", None)\n",
    "        example[\"input_ids\"], example[\"labels\"] = mask_tokens(\n",
    "            example[\"input_ids\"], mlm_probability, tokenizer, special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            example[\"labels\"][example[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "        example = {key: tf.convert_to_tensor(arr) for key, arr in example.items()}\n",
    "\n",
    "        yield example, {\"labels\": example[\"labels\"], \"next_sentence_label\": example[\"next_sentence_label\"]}  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "\n",
    "def mask_tokens(inputs, mlm_probability, tokenizer, special_tokens_mask):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "    labels = np.copy(inputs)\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "    probability_matrix = np.random.random_sample(labels.shape)\n",
    "    special_tokens_mask = special_tokens_mask.astype(np.bool_)\n",
    "\n",
    "    probability_matrix[special_tokens_mask] = 0.0\n",
    "    masked_indices = probability_matrix > (1 - mlm_probability)\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = (np.random.random_sample(labels.shape) < 0.8) & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = (np.random.random_sample(labels.shape) < 0.5) & masked_indices & ~indices_replaced\n",
    "    random_words = np.random.randint(low=0, high=len(tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64)\n",
    "    inputs[indices_random] = random_words\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sentence_pairing(examples):\n",
    "    first_seqs = []\n",
    "    nsp_labels = []\n",
    "\n",
    "    examples[\"log\"] = [\n",
    "        line for line in examples[\"log\"] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    \n",
    "    log_list = list(examples['log'])\n",
    "    for idx in range(len(log_list)):\n",
    "        first_value = log_list[idx]\n",
    "        if random.random() > 0.5:\n",
    "            # Pair with proper following log sequence\n",
    "            second_value = log_list[(idx + 1) % len(log_list)]\n",
    "\n",
    "            # IsNext Label\n",
    "            nsp_labels.append(0)\n",
    "        else:\n",
    "            # Pair with random log\n",
    "            rand_idx = random.randint(0, len(log_list) - 1)\n",
    "            second_value = log_list[rand_idx]\n",
    "\n",
    "            # IsNotNext Label\n",
    "            nsp_labels.append(1)\n",
    "\n",
    "        first_seqs.append((first_value, second_value))\n",
    "\n",
    "#     nsp_labels = tf.reshape(tf.constant(nsp_labels), (self.n_iter, self.batch_size))\n",
    "\n",
    "    return {\"log\": first_seqs, \"next_sentence_label\": nsp_labels}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    return w2vp.fast_tokenizer(\n",
    "        examples[\"log\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = data.map(\n",
    "    create_sentence_pairing,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"log\"],\n",
    "    desc=\"Running tokenizer on dataset line_by_line\"\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"log\"],\n",
    "    desc=\"Running tokenizer on dataset line_by_line\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(tokenized_datasets))), test_size=.30\n",
    ")\n",
    "\n",
    "eval_dataset = tokenized_datasets.select(val_indices)\n",
    "train_dataset = tokenized_datasets.select(train_indices)\n",
    "\n",
    "train_signature = {\n",
    "    feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "    for feature in train_dataset.features\n",
    "    if feature != \"special_tokens_mask\"\n",
    "}\n",
    "train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "train_signature = (train_signature, {\"labels\": train_signature[\"labels\"], \"next_sentence_label\": train_signature[\"next_sentence_label\"]} )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_generator = partial(sample_generator, train_dataset, w2vp.fast_tokenizer)\n",
    "\n",
    "tf_train_dataset = (\n",
    "    tf.data.Dataset.from_generator(tokenized_generator, output_signature=train_signature)\n",
    "    .batch(batch_size=10 * 50, drop_remainder=True)\n",
    "    .repeat(int(5))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_generator = partial(sample_generator, eval_dataset, w2vp.fast_tokenizer)\n",
    "eval_signature = {\n",
    "    feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "    for feature in eval_dataset.features\n",
    "    if feature != \"special_tokens_mask\"\n",
    "}\n",
    "eval_signature[\"labels\"] = eval_signature[\"input_ids\"]\n",
    "eval_signature = (eval_signature, {\"labels\": eval_signature[\"labels\"], \"next_sentence_label\": eval_signature[\"next_sentence_label\"]})\n",
    "tf_eval_dataset = (\n",
    "    tf.data.Dataset.from_generator(eval_generator, output_signature=eval_signature)\n",
    "    .batch(batch_size=10 * 50, drop_remainder=True)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batches_per_epoch = len(train_dataset) // (10 * 50)\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=1e-4,\n",
    "    num_train_steps=int(5 * batches_per_epoch),\n",
    "    num_warmup_steps=2,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=0.1,\n",
    "    weight_decay_rate=0.01,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vp.BERT.compile(optimizer=optimizer, loss={\"loss\": w2vp.BERT.compute_loss})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Phobert(TFBertForPreTraining):\n",
    "\n",
    "    def train_step(self, input_data):\n",
    "        x, y = input_data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward Pass\n",
    "            loss = self.compute_loss(\n",
    "                                     {\"labels\": y[\"labels\"], \"next_sentence_label\": y[\"next_sentence_label\"]}, \n",
    "                                     (y_pred[\"prediction_logits\"], y_pred[\"seq_relationship_logits\"])\n",
    "            )\n",
    "        \n",
    "        # Computing Gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Updating the Weights    \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        print(f\"Metrics: {self.compiled_metrics.metrics}\")\n",
    "        print(f\"InputData: {len(input_data)}\")\n",
    "        print(f\"y_pred: {len(y_pred)}\")\n",
    "        \n",
    "#         output = {m.name: m.result for m in self.metrics}        \n",
    "#         return (output)\n",
    "        return {\"loss\": loss}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_config = BertConfig(\n",
    "    vocab_size=w2vp.bert_tokenizer.get_vocab_size(),\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8\n",
    ")\n",
    "\n",
    "phobert = Phobert(bert_config)\n",
    "phobert.resize_token_embeddings(len(w2vp.fast_tokenizer))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "phobert.compile(optimizer=optimizer, loss=phobert.compute_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with yaspin(text=\"\", color='blue') as sp:\n",
    "    history = phobert.fit(\n",
    "            tf_train_dataset, \n",
    "            validation_data=tf_eval_dataset,\n",
    "            epochs=int(5),\n",
    "            steps_per_epoch=len(train_dataset) // (50 * 10)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -- Unsupervised Learning Pipeline -- #\n",
    "\n",
    "'''\n",
    "Input: pd.DataFrame with batch_size number of rows \n",
    "Seq: \n",
    "    Normalize \n",
    "    Phraser\n",
    "    Clustering\n",
    "    Extract Unique Layers\n",
    "    BERT\n",
    "Returns: transformers.TFBertForPreTrainingOutput\n",
    "'''\n",
    "\n",
    "# --- SUBWORD TOKENIZER --\n",
    "w2vp = UnsupervisedLearningPipeline(preprocessing_config, epochs=10, batch_size=10, seq_length=60)\n",
    "training_outputs = w2vp.fit(dataset[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(training_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def square_ragged_tensors(examples, key):\n",
    "    if isinstance(examples, dict):\n",
    "        return  examples[key].to_tensor(0)\n",
    "    else:\n",
    "        return examples.to_tensor(0)\n",
    "\n",
    "def some_func(examples: tf.data.Dataset, is_ragged=False) -> tf.data.Dataset:\n",
    "    if is_ragged:\n",
    "        if isinstance(list(examples)[0], dict):\n",
    "            for k_idx in range(len(list(examples)[0].keys())):\n",
    "                key = list(list(examples)[0].keys())[k_idx]\n",
    "                if isinstance(list(examples)[0][key], tf.RaggedTensor):\n",
    "                    list(examples)[0][key] = examples.batch(2).map(lambda x: square_ragged_tensors(x, key)).unbatch()\n",
    "        else:\n",
    "            examples = examples.batch(2).map(square_ragged_tensors).unbatch()\n",
    "        \n",
    "    return examples\n",
    "        \n",
    "a = tf.constant([0, 1, 2, 3])\n",
    "b = tf.constant([0, 1, 2, 3, 5, 6])\n",
    "rg = tf.ragged.stack([a, b])\n",
    "ex = {\"input_ids\": rg, \"hello_world\": [\"goodbye\", \"world\"]}\n",
    "\n",
    "batch_encodings = tf.data.Dataset.from_tensor_slices(ex)\n",
    "\n",
    "list(some_func(batch_encodings, True))\n",
    "# ex[\"input_ids\"].to_tensor(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertviz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data['log']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = test_data.iloc[0]['log']\n",
    "sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = w2vp.bert_tokenizer.text_to_sequence(sentence).tokens\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_view(attention, tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vp.BERT.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "copy_bert = deepcopy(w2vp.BERT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "copy_bert.get_output_at"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vp.BERT.save(SOURCE + '/results/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITSJA5hWASDn",
    "0JtgL-iQkqj2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "LongRunTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}