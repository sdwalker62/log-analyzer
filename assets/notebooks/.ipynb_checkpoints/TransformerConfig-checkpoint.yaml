TransformerGlobalConfig:
    d_model: 512
    max_seq_len: 200
    storage_path: '/results/'
    global_training: True
    batch_size: 50
    activation: 'relu'
    epoch: 25
    logdir: '/results/optimus_prime/'
    
BERTLayerConfig:
    num_attention_heads: 8
    num_encoder_layers: 12
    dff: 2048
    max_seq_len: 200
    dropout_rate: 0.1
    load_model: False
    save_model: True
    training: True

HitAnomalyLayerConfig:
    num_attention_heads: 12
    num_encoder_layers: 3
    dff: 2048
    max_seq_len: 200
    dropout_rate: 0.1
    load_model: False
    save_model: True
    training: True
    