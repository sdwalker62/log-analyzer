{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d721b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fabb51c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.data.ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-62b31e4ecf24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.data.ops'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.data.ops.dataset_ops import BatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd00768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac9c0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "# def _tf_collate_batch2(\n",
    "#     examples: Union[Dataset, list],\n",
    "#     tokenizer\n",
    "#     ):\n",
    "#     \"\"\"Collate `examples` into a suitable input for TFBertForPreTraining. Any padding is handled prior to the function call.\"\"\"\n",
    "    \n",
    "#     if isinstance(examples, list):\n",
    "#         examples = Dataset.from_tensor_slices(l_test)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Tensorize if necessary.\n",
    "#     if isinstance(examples[0], (list, tuple)):\n",
    "#         examples = [tf.constant(e, dtype=tf.float64) for e in examples]\n",
    "\n",
    "#     # Check if padding is necessary.\n",
    "#     length_of_first = examples[0].shape[0]\n",
    "#     are_tensors_same_length = all(x.shape[0] == length_of_first for x in examples)\n",
    "#     if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "#         return tf.stack(examples, axis=0)\n",
    "\n",
    "#     # If yes, check if we have a `pad_token`.\n",
    "#     if tokenizer._pad_token is None:\n",
    "#         raise ValueError(\n",
    "#             \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "#             f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "#         )\n",
    "\n",
    "#     # Creating the full tensor and filling it with our data.\n",
    "#     max_length = max(x.shape[0] for x in examples)\n",
    "#     if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "#         max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "\n",
    "#     result = tf.fill([len(examples), max_length], tokenizer.pad_token_id)\n",
    "#     for i, example in enumerate(examples):\n",
    "#         if tokenizer.padding_side == \"right\":\n",
    "#             temp_result = result.numpy()\n",
    "#             temp_result[i, : example.shape[0]] = example\n",
    "#             result = tf.convert_to_tensor(temp_result)\n",
    "#         else:\n",
    "#             temp_result = result.numpy()\n",
    "#             result[i, -example.shape[0] :] = example\n",
    "#             result = tf.convert_to_tensor(temp_result)\n",
    "#     return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFDataCollatorForLanguageModeling2:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to use masked language modeling. If set to :obj:`False`, the labels are the same as the\n",
    "            inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for\n",
    "            non-masked tokens and the value to predict for the masked token.\n",
    "        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when :obj:`mlm` is set to :obj:`True`.\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n",
    "        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n",
    "        argument :obj:`return_special_tokens_mask=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "\n",
    "    def __call__(self, examples: Union[List[Union[List[int], tf.Tensor, Dict[str, tf.Tensor]]], Dataset]) -> Dataset:\n",
    "        \n",
    "        if not isinstance(examples, Dataset):\n",
    "            if isinstance(examples[0], dict):\n",
    "                examples = Dataset.from_tensor_slices([x['input_ids'] for x in test_list_dict])                \n",
    "            else:\n",
    "                examples = Dataset.from_tensor_slices(examples)\n",
    "        \n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = tf.identity(batch[\"input_ids\"])\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels = labels.numpy()\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "                labels = tf.constant(labels)\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def tf_mask_tokens(\n",
    "        self, inputs: tf.Tensor, special_tokens_mask: Optional[tf.Tensor] = None\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = tf.identity(inputs)\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = tf.fill(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in list(labels)\n",
    "            ]\n",
    "            special_tokens_mask = tf.cast(tf.constant(special_tokens_mask, dtype=tf.float16), dtype=tf.bool)\n",
    "        else:\n",
    "            special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.bool)\n",
    "\n",
    "        probability_matrix = tf.where(~special_tokens_mask, probability_matrix, 0)\n",
    "        masked_indices = tfp.distributions.Bernoulli(probs=probability_matrix, dtype=tf.bool).sample()\n",
    "\n",
    "        labels = tf.where(masked_indices, labels, -100)  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "\n",
    "        indices_replaced = (\n",
    "            tfp.distributions.Bernoulli(probs=tf.fill(labels.shape, 0.8), dtype=tf.bool).sample() & masked_indices\n",
    "        )\n",
    "\n",
    "        mask_token = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        inputs = tf.where(~indices_replaced, inputs, mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = (\n",
    "            tfp.distributions.Bernoulli(probs=tf.fill(labels.shape, 0.5), dtype=tf.bool).sample()\n",
    "            & masked_indices\n",
    "            & ~indices_replaced\n",
    "        )\n",
    "\n",
    "        random_words = tf.random.uniform(labels.shape, maxval=len(self.tokenizer), dtype=tf.float64)\n",
    "\n",
    "        inputs = tf.cast(inputs, dtype=tf.float64)\n",
    "        inputs = tf.where(indices_random, random_words, inputs)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "630a31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9, 8, 6, 8, 4]),\n",
       " array([8, 9, 4, 1, 3]),\n",
       " array([7, 2, 4, 3, 2]),\n",
       " array([5, 0, 3, 2, 9])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_test = list(np.random.randint(0, 10, (4,5)))\n",
    "l_test = tf.data.Dataset.from_tensor_slices(l_test)\n",
    "l_test = list(l_test.as_numpy_iterator())\n",
    "l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1896b25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = list(dataset.as_numpy_iterator())\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd4cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertConfig\n",
    "from transformers import TFBertForPreTraining\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from bertviz import head_view, model_view\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# -- Tokenizer -- #\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from tokenizers.normalizers import (\n",
    "    Lowercase,\n",
    "    NFD,\n",
    "    StripAccents\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimeTokenizer:\n",
    "    def __init__(self, max_seq_length: int):\n",
    "        self.prime_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "        self.prime_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "        self.prime_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        self.prime_tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[CLS] $A [SEP]\",\n",
    "            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "            special_tokens=[\n",
    "                (\"[CLS]\", 1),\n",
    "                (\"[SEP]\", 2),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.trainer = WordPieceTrainer(\n",
    "            vocab_size=153411, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "        self.prime_tokenizer.decoder = decoders.WordPiece()\n",
    "        # self.prime_tokenizer.enable_padding(length=max_seq_length)\n",
    "        # self.prime_tokenizer.enable_truncation(max_seq_length)\n",
    "\n",
    "    def text_to_sequence(self, input_):\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.encode_batch(input_)\n",
    "        return self.prime_tokenizer.encode(input_)\n",
    "\n",
    "    def sequence_to_text(self, input_):\n",
    "        if type(input_) is list:\n",
    "            return self.prime_tokenizer.decode_batch(batch)\n",
    "        return self.prime_tokenizer.decode(input_)\n",
    "\n",
    "    def train(self, data):\n",
    "        log_itr = iter(data)\n",
    "#         tqdm_log_itr = tqdm(iterable=log_itr, total=len(data))\n",
    "# tqdm_log_itr.__iter__()\n",
    "        self.prime_tokenizer.train_from_iterator(log_itr, self.trainer)\n",
    "        self.save()\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.prime_tokenizer\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.prime_tokenizer.get_vocab()\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.prime_tokenizer.get_vocab_size()\n",
    "    \n",
    "    def save(self):\n",
    "        self.prime_tokenizer.save(\"prime_tokenizer.json\")\n",
    "        \n",
    "    def load(self):\n",
    "        self.prime_tokenizer = Tokenizer.from_file(\"prime_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08d95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PrimeTokenizer(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b009c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"00000067:solr-1.clicls[0016:adfd]\", \"00000064:solr-1.srvcls[0020:adfd]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bbec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49faec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "the_tokenizer_obj = tokenizer.get_tokenizer()\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=the_tokenizer_obj)\n",
    "fast_tokenizer.unk_token = \"[UNK]\"\n",
    "fast_tokenizer.sep_token = \"[SEP]\"\n",
    "fast_tokenizer.pad_token = \"[PAD]\"\n",
    "fast_tokenizer.cls_token = \"[CLS]\"\n",
    "fast_tokenizer.mask_token = \"[MASK]\"\n",
    "batch_encodings = fast_tokenizer(corpus, truncation=True, padding=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a759a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 64, 13, 54, 5, 8, 6, 65, 14, 62, 13, 53, 15, 2], [1, 63, 13, 54, 5, 8, 6, 66, 14, 61, 13, 53, 15, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5ca119d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 64, 13, 54, 5, 8, 6, 65, 14, 62, 13, 53, 15, 2],\n",
       " [1, 64, 13, 54, 5, 8, 6, 65, 14, 62, 13, 53, 15, 2],\n",
       " [1, 63, 13, 54, 5, 8, 6, 66, 14, 61, 13, 53, 15, 2],\n",
       " [1, 63, 13, 54, 5, 8, 6, 66, 14, 61, 13, 53, 15, 2]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer.text_to_sequence(corpus)\n",
    "\n",
    "batch_ids = [log.ids for log in batch for i in range(2)]\n",
    "\n",
    "batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5ff59531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4, 14), dtype=int32, numpy=\n",
       "array([[[ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2]],\n",
       "\n",
       "       [[ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2]],\n",
       "\n",
       "       [[ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2]],\n",
       "\n",
       "       [[ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 64, 13, 54,  5,  8,  6, 65, 14, 62, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2],\n",
       "        [ 1, 63, 13, 54,  5,  8,  6, 66, 14, 61, 13, 53, 15,  2]]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ids = tf.convert_to_tensor([batch_ids, batch_ids, batch_ids, batch_ids ])\n",
    "batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b1665430",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TFDataCollatorForLanguageModeling(fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07c5c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 14), dtype=float64, numpy=\n",
       " array([[ 1.        ,  4.        , 13.        ,  4.        ,  4.        ,\n",
       "          8.        ,  6.        , 65.        , 14.        , 62.        ,\n",
       "         13.        , 53.        , 15.        ,  2.        ],\n",
       "        [ 1.        , 64.        , 13.        ,  4.        ,  5.        ,\n",
       "          8.        ,  6.        , 65.        , 14.        , 62.        ,\n",
       "         13.        , 53.        , 15.        ,  2.        ],\n",
       "        [ 1.        , 63.        ,  4.        , 54.        ,  4.        ,\n",
       "          8.        ,  6.        , 66.        , 25.24303333, 61.        ,\n",
       "         13.        , 53.        , 15.        ,  2.        ],\n",
       "        [ 1.        , 63.        , 13.        , 54.        ,  5.        ,\n",
       "          8.        ,  6.        , 66.        , 14.        , 61.        ,\n",
       "         13.        , 53.        ,  4.        ,  2.        ]])>,\n",
       " 'labels': <tf.Tensor: shape=(4, 14), dtype=float64, numpy=\n",
       " array([[-100.,   64., -100.,   54.,    5., -100., -100., -100., -100.,\n",
       "         -100., -100., -100., -100., -100.],\n",
       "        [-100., -100., -100.,   54., -100., -100., -100., -100., -100.,\n",
       "         -100., -100., -100., -100., -100.],\n",
       "        [-100., -100.,   13., -100.,    5., -100., -100., -100.,   14.,\n",
       "         -100., -100., -100., -100., -100.],\n",
       "        [-100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
       "         -100., -100., -100.,   15., -100.]])>}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77f73821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (14,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(batch_ids)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8e155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c764c1f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    <ipython-input-108-8f204293394e>:1 None  *\n        lambda x: data_collator([x])\n    <ipython-input-2-fafe5a402890>:112 __call__  *\n        batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n    <ipython-input-2-fafe5a402890>:134 tf_mask_tokens  *\n        special_tokens_mask = [\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:505 __iter__  **\n        self._disallow_iteration()\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:501 _disallow_iteration\n        self._disallow_in_graph_mode(\"iterating over `tf.Tensor`\")\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:479 _disallow_in_graph_mode\n        raise errors.OperatorNotAllowedInGraphError(\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-8f204293394e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4203\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4204\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3049\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[0;32m-> 3051\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3052\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    <ipython-input-108-8f204293394e>:1 None  *\n        lambda x: data_collator([x])\n    <ipython-input-2-fafe5a402890>:112 __call__  *\n        batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n    <ipython-input-2-fafe5a402890>:134 tf_mask_tokens  *\n        special_tokens_mask = [\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:505 __iter__  **\n        self._disallow_iteration()\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:501 _disallow_iteration\n        self._disallow_in_graph_mode(\"iterating over `tf.Tensor`\")\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:479 _disallow_in_graph_mode\n        raise errors.OperatorNotAllowedInGraphError(\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: data_collator([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2143ea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
